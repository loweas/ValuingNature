---
format: html
editor: visual
---

# Stated Preference

**Core Stated Preference Methods:**

-   **Contingent Valuation**

    This method directly asks individuals about their willingness to pay for a good or service, or their willingness to accept compensation for its loss. In R, packages can be used to analyze responses from single-bounded or double-bounded dichotomous choice CV surveys, employing both parametric and nonparametric approaches.

-   **Discrete Choice**

    DCEs present respondents with several choice scenarios, each containing multiple alternatives described by various attributes and their levels. Respondents choose their preferred alternative in each scenario. R packages facilitate the design of DCEs (e.g., using orthogonal main-effect designs) and the analysis of choice data using models like conditional and binary logit.

# Contingent Valuation

The contingent valuation method involves directly asking people, in a survey, how much they would be willing to pay for specific environmental services.  In some cases, people are asked for the amount of compensation they would be willing to accept to give up specific environmental services.  It is called **contingent valuation**, because people are asked to state their willingness to pay, *contingent* on a specific hypothetical scenario and description of the environmental service.

## Example

Based on [James Fogarty and Hideo Aizaki](https://nmvr.skr.jp/01-cv1.html#ref-ecdat)

### Packages

Open R. You will need additional packages **DCchoice**, **Ecdat** ([Croissant and Graves 2020](#0)) and **lmtest** ([Zeileis and Hothorn 2002](#0)).

```{r}
#install.packages("DCchoice",repos = c("http://www.bioconductor.org/packages/release/bioc","https://cran.rstudio.com/"), dep = TRUE)

#install.packages(ggplot2)
#install.packages(dplyr)
```

After installing the necessary packages, the next step is to load `DCchoice`, `Ecdat`, and `lmtest` into your current R session. The `DCchoice` package provides the core functions used in our analysis. Additionally, `lmtest` offers essential tools for model testing. The `Ecdat` package includes publicly available real-world datasets.

*For this example, we'll use the `NaturalParks` dataset to demonstrate how contingent valuation (CV) study data can be analyzed.*

```{r echo=TRUE, message=FALSE, warning=FALSE, results='hide'}
library(DCchoice)
library(Ecdat)
library(lmtest)
library(ggplot2)
library(dplyr)
```

### Load Data

We will use a built-in sample dataset from the `Ecdat` package. After loading the dataset, you can use the `head()` function to preview its contents. By default, `head()` displays the first six rows along with the column names, but you can adjust the number of rows shown by using the `n` argument. This function is a quick way to get an overview of the dataset's structure.

```{r}
# Load the data from Ecdat package
data(NaturalPark, package = "Ecdat") 

# Display the first three rows of data          
head(NaturalPark, n = 5)                     
```

### Data Variables

The **NaturalParks (NP)** dataset contains seven variables, each representing a specific aspect of the survey responses:

-   **`bid1`**: The initial bid amount (in euros) presented in the first willingness-to-pay (WTP) question. In this dataset, there are four possible bid amounts: 6, 12, 24, and 48.

-   **`bidh`**: The higher follow-up bid amount, shown only if the respondent answered "yes" to the initial WTP question.

-   **`bidl`**: The lower follow-up bid amount, shown only if the respondent answered "no" to the initial WTP question.

-   **`answers`**: A factor variable representing the respondent's answers to the two WTP questions. The four possible combinations are:

    -   `nn`: no to both

    -   `ny`: no to first, yes to second

    -   `yn`: yes to first, no to second

    -   `yy`: yes to both\

        Since there are four combinations, this factor variable has four levels.

-   **`age`**: Age is grouped into six brackets rather than exact values. Higher bracket numbers correspond to older respondents.

-   **`sex`**: A factor variable with two levels: "male" and "female."

-   **`income`**: Income is also grouped into eight brackets. As with age, higher numbers represent higher income levels.

### Data Manipulation

The decision identifies what each individual prefers. This is indicated by the "yes" and "no." The variable needs to be converted into a numerical representation. To do this you will need to write a loop and create a binary indicator for when the individuals says yes to the first bid. This is indicated by the first letter of the variable starting with "y"

```{r}
NaturalPark$ans1 <- ifelse(NaturalPark$answers == "yy" | NaturalPark$answers == "yn", 1, 0)

# Display the first three rows of data          
head(NaturalPark, n = 5)   
```

Now we want to count the number of individuals who said yes for each price. This table shows the rows as yes or no for first bid and all the prices.

```{r}
table(NaturalPark$ans1, NaturalPark$bid1) 

```

The table can be interpreted as follows:

-   For a bid price of **€6**, there are **76 survey responses**. Out of these, **50 respondents** said they would be willing to pay €6, while **26 said they would not**.

-   And so on for higher bid amounts.

As the **bid price increases**, we expect rationally (from law of demand) there will be a **decline in the proportion of "yes" responses**, since fewer people are likely to be willing to pay higher amounts for the same environmental improvement.\

This pattern is visible in the table, particularly when reading across the row representing "yes" responses (often coded as `1`), where the number of affirmative answers drops from **50 to 36**, and continues decreasing with higher bids. Lets look at this in a graph.

### Data Visualization

To create our demand line, we will first have to convert the data in the right shape form.

```{r}

yes_data <- as.data.frame(table(NaturalPark$ans1, NaturalPark$bid1)) %>%
  rename(Answer = Var1, Bid = Var2, Count = Freq) %>%
  filter(Answer == 1) %>%
  mutate(Bid = as.numeric(as.character(Bid)))

# Display the first three rows of data          
head(yes_data, n = 5)  

```

Now we will use ggplot and visually show how people respond to bid prices:

```{r}
ggplot(yes_data, aes(x = Count, y = Bid)) +
  geom_line(color = "darkgreen", size = 1.2) +
  geom_point(color = "darkgreen", size = 3) +
  scale_y_continuous(breaks = yes_data$Bid) +
  labs(
    title = "Decline in 'Yes' Responses with Higher Bid Prices",
    x = "Number of 'Yes' Responses",
    y = "Bid Amount (€)"
  ) +
  theme_minimal()

```

And voilà! There we have a downward sloping demand curve!

People respond to prices which makes sense and suggest that people are responding in a way that they do in reality.

### Model Estimation

Now that we have explored the data and tested peoples rational. We are going to estimate the willingness to pay for the contingent valuation question.

For now, we'll ignore gender, income, and age. The model assumes that a person's response :

-   ***0** ("no, not willing to pay")*

-   ***1** ("yes, willing to pay") depends only on the **bid amount** (or "price").*

So this model includes just one explanatory variable which is the bid.

Using **DCchoice** the formula for this type of model is as follows:

`model1 <- sbchoice(dep var ~ exp var | bid, dist = "", data = my.data)`

Here's what each part means:

-   `model1`: The name we give to save the model results (for example, `sb1` for the first model).

    `<-`: The assignment symbol that stores the model in `model1`.

-   `sbchoice`: The function that runs the model.

-   `dep_var`: The dependent variable (the responses coded as 0 = no, 1 = yes). For our first model this is `ans1`.

-   `exp_var`: The explanatory variables *other than* the bid amount. Since we don't have any for the first model, we just use `1` for the intercept.

-   `| bid`: This part specifies the bid amount variable, which is `bid1` in our example.

-   `dist = ""`: Specifies the distribution used; we'll always use logistic, so it will be `dist = "logistic"`

-   `data = my.data`: The data frame that contains the data which in our case is `NaturalPark`.

After fitting the model, we usually check the results with the `summary()` function. To focus on the coefficient estimates, we can use `coeftest()` from the **lmtest** package.

```{r}
# A very simple model
sb1 <- sbchoice(ans1 ~ 1 | bid1,  dist = "logistic", data = NaturalPark)
coeftest(sb1)

```

The most important thing to notice in the model's coefficients is the **negative sign** on the bid variable. No matter how the original data is coded, the bid will always show up as **BID** in the results. This negative sign means that as the bid price goes up, fewer people say "yes" they are less willing to pay that amount!

The results also show that this relationship is **statistically significant**, meaning it's unlikely to be due to chance.

Lets see these results visually by using the `plot()` function. This plot shows the willingness-to-pay data along with a horizontal line marking the point where 50% of people say "yes." From the plot, you can see that this 50% support happens at about **€35**.

```{r}
# plots the predicted support at each bid value 
plot(sb1, las = 1)  

# adds a horizontal line to the plot 
abline(h = 0.5, lty = 2, col = "red") 
```

### Full Estimation

We now look at the full model summary output we get when we use the `sbchoice()` function:

```{r}
#  Model summary
summary(sb1) 
```

The summary output has several parts:

-   The model we estimated:

    ```         
    Formula:
    ans1 ~ 1 | bid1

    Coefficients: 
                 Estimate Std. Error z value Pr(>|z|)   
    (Intercept)  0.550045   0.199874   2.752  0.00592 **
    BID         -0.015722   0.007181  -2.190  0.02856 * 
    ---
    Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
    ```

-   The second part shows the model's coefficients and whether they are statistically significant.

    ```         
    Distribution: logistic 
    Number of Obs.: 312  
    log-likelihood: -212.3968 
    pseudo-R^2: 0.0113 , adjusted pseudo-R^2: 0.0020 
    LR statistic: 4.841 on 1 DF, p-value: 0.028 
    AIC: 428.793653 , BIC: 436.279659 
    ```

-   The third part shows how well the model fits the data. This helps us compare different models. It's important to see "Convergence: TRUE," which means the model worked properly. If not, you'll get an error.

    ```         
    Iterations: 4  
    Convergence: TRUE 
    ```

-   The fourth part shows the estimated average and median willingness to pay (WTP). Lets take a moment here

    ```         
    WTP estimates:
     Mean : 63.95526 
     Mean : 26.04338 (truncated at the maximum bid)
     Mean : 47.26755 (truncated at the maximum bid with adjustment)
     Median : 34.98512 
    ```

#### 1. Mean WTP

**Value:** **Value:** `63.95`

This estimate can be calculated by taking the coefficient of the estimation on bid:

$1/\beta_{bid} = 1/-0.015722=63.95$

This is the **unbounded mean** and its calculated using the full range of the estimated distribution of WTP, even beyond your maximum bid.

**How it's calculated:**

-   Uses the model's estimated coefficients to integrate the WTP distribution from 0 to infinity (or theoretically negative infinity to positive infinity depending on the model).

-   **Issue:** May overestimate WTP, especially if the upper tail is long (which it often is).

```{r}
medianWTP=1/sb1$coefficients[2]
medianWTP
```

#### 2. **Mean WTP (truncated at max bid)**

**Value:** `26.04`\

This is the mean WTP **truncated at the maximum bid** in your data.

$1/\beta_{bid}(1+exp(\beta_0+\sum(\beta_nZ_n)))$

**How it's calculated:**

-   Integration is limited from 0 up to the **highest bid amount** offered in your survey.

-   **Why:** This avoids overestimating WTP from extrapolating too far beyond your data.

```{r}

medianWTP * (log(1 + exp(sb1$coefficients[1] + sb1$coefficients[2] * max(NaturalPark$bid1))) - log(1 + exp(sb1$coefficients[1])))

```

#### 3. **Mean WTP (truncated with adjustment)**

**Value:** `47.27`\

This is the **same as #2** but with a **correction** applied to adjust for truncation bias.

**How it's calculated:**

-   Still truncated at the max bid, but includes a **bias-correction factor** based on the distribution. the truncated mean WTP with the adjustment of Boyle et\~al.(1988)

-   **Why:** Balances realism (truncation) with statistical accuracy (adjustment).

#### 4. **Median WTP**

**Value:** `34.99`\

This is the **bid level at which 50% of respondents are predicted to say "yes"**.

**How it's calculated:**

$-(\alpha/\beta_{bid})$

-   It's the bid where the predicted probability of saying "yes" is 0.5, based on your model.

-   **Why:** Commonly used because it's less sensitive to skewed data than the mean.

```{r}
-sb1$coefficients[1]/sb1$coefficients[2]

```

# Discrete Choice

For this example we will be using this book

[Environmental Valuation with Discrete Choice Experiments in R](https://library.oapen.org/bitstream/handle/20.500.12657/104194/1/9783031893384.pdf)

These are the libraries you need to run the code below:

```{r}


library(spdesign)



```

We will use the example the above book uses through out the chapters.

## Attribute & Levels

+------------------------------------+------------------+-------------------+
| Attributes                         | Labels           | Levels            |
+====================================+==================+===================+
| Size of Wind Farm (discrete)       | Small Farms      | 0                 |
+------------------------------------+------------------+-------------------+
| ***Note reference is LargeFarm***  |                  | 1                 |
+------------------------------------+------------------+-------------------+
|                                    | MediumFarms      | 0                 |
+------------------------------------+------------------+-------------------+
|                                    |                  | 1                 |
+------------------------------------+------------------+-------------------+
| Max. Height Turbine (discrete)     | Low Height       | 0                 |
+------------------------------------+------------------+-------------------+
| ***Note reference is HighHeight*** |                  | 1                 |
+------------------------------------+------------------+-------------------+
|                                    | Medium Height    | 0                 |
+------------------------------------+------------------+-------------------+
|                                    |                  | 1                 |
+------------------------------------+------------------+-------------------+
| Reduction in Red Kite (continous)  | Red Kite         | 5                 |
+------------------------------------+------------------+-------------------+
|                                    |                  | 7.5               |
+------------------------------------+------------------+-------------------+
|                                    |                  | 10                |
+------------------------------------+------------------+-------------------+
|                                    |                  | 12.5              |
+------------------------------------+------------------+-------------------+
|                                    |                  | 15                |
+------------------------------------+------------------+-------------------+
| Distance to residents (continous)  | MinDistance      | 750               |
+------------------------------------+------------------+-------------------+
|                                    |                  | 1000              |
+------------------------------------+------------------+-------------------+
|                                    |                  | 1250              |
+------------------------------------+------------------+-------------------+
|                                    |                  | 1500              |
+------------------------------------+------------------+-------------------+
|                                    |                  | 1750              |
+------------------------------------+------------------+-------------------+
| MonthlyCost                        | Cost             | 0                 |
|                                    |                  |                   |
| (Continous)                        |                  |                   |
+------------------------------------+------------------+-------------------+
|                                    |                  | 1                 |
+------------------------------------+------------------+-------------------+
|                                    |                  | 2                 |
+------------------------------------+------------------+-------------------+
|                                    |                  | 3                 |
+------------------------------------+------------------+-------------------+
|                                    |                  | 4                 |
+------------------------------------+------------------+-------------------+
|                                    |                  | .....             |
+------------------------------------+------------------+-------------------+
|                                    |                  | ....              |
+------------------------------------+------------------+-------------------+
|                                    |                  | 10                |
+------------------------------------+------------------+-------------------+

## Choice Set

Lets first consider this example

![](images/Screenshot%202025-09-20%20at%205.35.39%20PM.png)

This choice card has 3 alternatives and thus 3 different utility functions you are estimating:

-   The status quo or the opting out and keeping things the way they are. The utility function would look something like this:

$$
U_{n1t}= 
\beta_{mf}Med.Farms_{n1t}+\beta_{sf}SmallFarms_{n1t} \\ 
+\beta_{mh}Med.Height_{n1t}+\beta_{lh}LowHeight_{n1t} \\
+\beta_{rk}redKite_{n|t}+\beta_{md}MinDistance_{n1t} \\ 
\beta_{cost}Cost_{n1t}+ \epsilon_{n1t}
$$

-   Program B will be alternative 2 and thus is indexed by 2

$$
U_{n2t}= \beta_{mf}Med.Farms_{n2t}+\beta_{sf}SmallFarms_{n2t} \\ 
+\beta_{mh}Med.Height_{n2t}+\beta_{lh}LowHeight_{n2t} \\
+\beta_{rk}redKite_{n2t}+\beta_{md}MinDistance_{n2t} \\ 
\beta_{cost}Cost_{n2t}+ \epsilon_{n2t}
$$

-   Program C will be alternative 3 and thus is indexed by 3

    $$
    U_{n3t}= \beta_{mf}Med.Farms_{n3t}+\beta_{sf}SmallFarms_{n3t} \\ 
    +\beta_{mh}Med.Height_{n3t}+\beta_{lh}LowHeight_{n3t} \\
    +\beta_{rk}redKite_{n3t}+\beta_{md}MinDistance_{n3t} \\ 
    \beta_{cost}Cost_{n3t}+ \epsilon_{n3t}
    $$

## Experiential Design

We will now look at a full factorial design for the entire choice set and all the levels. Given the choices above the amount of possible combinations balloons to 5mil+ observations!

```{r}
# Create the full factorial using a named list of attributes and levels in the wide format
full_fact <- full_factorial( list( alt1_sq = 1,
alt1_farm = 0,
alt1_height = 0,
alt1_redkite = 0,
alt1_distance = 0,
alt1_cost = 0,
alt2_sq = 0,
alt2_farm = c(1, 2, 3),
alt2_height = c(1, 2, 3),
alt2_redkite = c(-5, -2.5, 0, 2.5, 5), alt2_distance = c(0, 0.25, 0.5, 0.75, 1), alt2_cost = 1:10,
alt3_sq = 0,
alt3_farm = c(1, 2, 3),
alt3_height = c(1, 2, 3),
alt3_redkite = c(-5, -2.5, 0, 2.5, 5), alt3_distance = c(0, 0.25, 0.5, 0.75, 1), alt3_cost = 1:10
) )

# Show the first six rows and 8th to 12th columns of the design matrix
full_fact[1:6, c(1, 8:12)]
```

As the number of attributes, levels, and alternatives increases, full factorial designs become less practical for several reasons:

1.  **Duplicate alternatives**: Some choice tasks may repeat the same alternative, which doesn't help us learn anything new about preferences.

2.  **Dominated alternatives**: Some options in a choice task might be clearly worse (or better) than others in every way. These don't help reveal trade-offs because people will always pick the best one, making the data less useful.

3.  **Lack of control**: The full factorial includes all possible combinations, even unrealistic ones. For example, we might want to prevent small wind farms from showing up with the highest red kite impact.

### Logical Operators

Lets say we want to put restriction by putting logical restrictions. For example, the tall windmills cannot be placed too close to residential areas (this could already be a law and thus is a more accurate reflection of reality).

```{r}

candidate_set <- full_fact[!((full_fact$alt2_height == 1 & full_fact$alt2_distance < 0.75) | (full_fact$alt3_height == 1 & full_fact$alt3_distance < 0.75)), ]

candidate_set[1:6, c(1, 8:12)]
```

This reduces the number of observations to 3.2million+ but does not make our choice set reasonable for population sampling. There for we move onto the next approach D-efficient

### D-efficient Design

In statistics, we often try to reduce standard errors to improve the precision of our estimates. The same idea applies in Discrete Choice Experiments (DCEs). We want to design choice tasks that give us the most precise information.

Think of it this way:

-   When **fitting a model**, we already have the data and estimate the parameters that best explain it.

-   When **designing a DCE**, we do the reverse: we assume values for the parameters (called *priors*) and then search for the combination of attributes and levels that will give us the most information --- that is, the **lowest standard errors** or **lowest D-error**.

### Utility Function

For our example we need to design a utility function to estimate the best set of potential choice cards. The utility function was written out within choice set section. So here we are going to use the library `spdesign` to write out each alternative.

```{r}
utility <- list(
alt1 = "b_sq[0] * sq[1]",
alt2 = "b_farm_dummy[c(0.25, 0.5)] * farm[c(1, 2, 3)] +
b_height_dummy[c(0.25, 0.5)] * height[c(1, 2, 3)] + b_redkite[-0.05] * redkite[c(-5, -2.5, 0, 2.5, 5)] + b_distance[0.5] * distance[c(0, 0.25, 0.5, 0.75, 1)] + b_cost[-0.05] * cost[seq(1, 10)]",
alt3 = "b_farm_dummy * farm + b_height_dummy * height +
b_redkite * redkite + b_distance * distance + b_cost * cost"
)
				
```

### Generating Design

In library **spdesign** `generate_design`is a function that generates efficient experimental designs. The function takes a set of indirect utility functions and generates efficient experimental designs assuming that people are maximizing utility.

Here are the arguments needed for our example:

|                       |                                                                                                                                                                                                                                                                |
|-----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `utility`             | A named list of utility functions. See the examples and the vignette for examples of how to define these correctly for different types of experimental designs.                                                                                                |
| `rows`                | An integer giving the number of rows in the final design                                                                                                                                                                                                       |
| `model`               | A character string indicating the model to optimize the design for. Currently the only model programmed is the 'mnl' model and this is also set as the default.                                                                                                |
| `efficiency_criteria` | A character string giving the efficiency criteria to optimize for. One of 'a-error', 'c-error', 'd-error' or 's-error'. No default is set and argument must be specified. Optimizing for multiple criteria is not yet implemented and will result in an error. |
| `algorithm`           | A character string giving the optimization algorithm to use. No default is set and the argument must be specified to be one of 'rsc', 'federov' or 'random'.                                                                                                   |

```{r}
# Generate design ----
design <- generate_design(utility, rows = 100,
model = "mnl", efficiency_criteria = "d-error", algorithm = "rsc")
```

```{r}
summary(design)
```

### Correlation 

Next step check correlation

```{r}
# Correlation matrix
cor(design)
```

### Attribute Balance

```{r}
# Print only the first three list elements
level_balance(design)[1:3]
```

First, we can see that the constant for the status quo alternative appears in all 100 rows of the design. Next, the medium and small wind farm sizes each occur 33 times, meaning the large size appears 34 times. This suggests the design is nearly balanced across attribute levels.

### Dominated Strategy Check

**Dominant** or **dominated** alternatives should be avoided because they don\'t provide useful information about trade-offs and can bias your results.

To check for this, we can look at the **choice probabilities** for each alternative. If one option has a probability close to **1**, it likely dominates the others. If it\'s close to **0**, it\'s probably dominated.

The `spdesign` package includes a `probabilities()` function that calculates these values based on your design and priors. It shows the probability of choosing each alternative in every choice task. Each row of the output adds up to 1.

```{r}

# Check the utility balance by inspecting the probabilities. We use head() to avoid printing all 100 rows in the book.
probabilities(design) |>
head()
```

