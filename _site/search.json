[
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Contingent Valuation",
    "section": "",
    "text": "The contingent valuation method involves directly asking people, in a survey, how much they would be willing to pay for specific environmental services.  In some cases, people are asked for the amount of compensation they would be willing to accept to give up specific environmental services.  It is called contingent valuation, because people are asked to state their willingness to pay, contingent on a specific hypothetical scenario and description of the environmental service. This page will go through an adapted version based on James Fogarty and Hideo Aizaki\n\n\nOpen R. You will need additional packages\n\nDCchoice (Nakatani and Aizaki, n.d.)Functions for analyzing dichotomous choice contingent valuation (CV) data. It provides functions for estimating parametric and nonparametric models for single-, one-and-one-half-, and double-bounded CV data. For details, see (Aizaki et al. 2022)\nEcdat (Croissant and Graves, n.d.) For the dataset NationalPark: Willingness to Pay for the Preservation of the Alentejo Natural Park\nlmtest (Hothorn et al., n.d.)\n\n\n\nShow code\n#install.packages(\"DCchoice\",repos = c(\"http://www.bioconductor.org/packages/release/bioc\",\"https://cran.rstudio.com/\"), dep = TRUE)\n\n#install.packages(ggplot2)\n#install.packages(dplyr)\n\n\nAfter installing the necessary packages, the next step is to load DCchoice, Ecdat, and lmtest into your current R session. The DCchoice package provides the core functions used in our analysis. Additionally, lmtest offers essential tools for model testing. The Ecdat package includes publicly available real-world datasets.\nFor this example, we’ll use the NaturalParks dataset to demonstrate how contingent valuation (CV) study data can be analyzed.\n\n\nShow code\nlibrary(DCchoice)\nlibrary(Ecdat)\nlibrary(lmtest)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\n\n\n\nWe will use a built-in sample dataset from the Ecdat package. After loading the dataset, you can use the head() function to preview its contents. By default, head() displays the first six rows along with the column names, but you can adjust the number of rows shown by using the n argument. This function is a quick way to get an overview of the dataset’s structure.\n\n\nShow code\n# Load the data from Ecdat package\ndata(NaturalPark, package = \"Ecdat\") \n\n# Display the first three rows of data          \nhead(NaturalPark, n = 5)                     \n\n\n  bid1 bidh bidl answers age    sex income\n1    6   18    3      yy   1 female      2\n2   48  120   24      yn   2   male      1\n3   48  120   24      yn   2 female      3\n4   24   48   12      nn   5 female      1\n5   24   48   12      ny   6 female      2\n\n\n\n\n\nThe NaturalParks (NP) dataset contains seven variables, each representing a specific aspect of the survey responses:\n\nbid1: The initial bid amount (in euros) presented in the first willingness-to-pay (WTP) question. In this dataset, there are four possible bid amounts: 6, 12, 24, and 48.\nbidh: The higher follow-up bid amount, shown only if the respondent answered “yes” to the initial WTP question.\nbidl: The lower follow-up bid amount, shown only if the respondent answered “no” to the initial WTP question.\nanswers: A factor variable representing the respondent’s answers to the two WTP questions. The four possible combinations are:\n\nnn: no to both\nny: no to first, yes to second\nyn: yes to first, no to second\nyy: yes to both\n\nSince there are four combinations, this factor variable has four levels.\n\nage: Age is grouped into six brackets rather than exact values. Higher bracket numbers correspond to older respondents.\nsex: A factor variable with two levels: “male” and “female.”\nincome: Income is also grouped into eight brackets. As with age, higher numbers represent higher income levels.\n\n\n\n\nThe decision identifies what each individual prefers. This is indicated by the “yes” and “no.” The variable needs to be converted into a numerical representation. To do this you will need to write a loop and create a binary indicator for when the individuals says yes to the first bid. This is indicated by the first letter of the variable starting with “y”\n\n\nShow code\nNaturalPark$ans1 &lt;- ifelse(NaturalPark$answers == \"yy\" | NaturalPark$answers == \"yn\", 1, 0)\n\n# Display the first three rows of data          \nhead(NaturalPark, n = 5)   \n\n\n  bid1 bidh bidl answers age    sex income ans1\n1    6   18    3      yy   1 female      2    1\n2   48  120   24      yn   2   male      1    1\n3   48  120   24      yn   2 female      3    1\n4   24   48   12      nn   5 female      1    0\n5   24   48   12      ny   6 female      2    0\n\n\nNow we want to count the number of individuals who said yes for each price. This table shows the rows as yes or no for first bid and all the prices.\n\n\nShow code\ntable(NaturalPark$ans1, NaturalPark$bid1) \n\n\n   \n     6 12 24 48\n  0 26 34 40 41\n  1 50 43 42 36\n\n\nThe table can be interpreted as follows:\n\nFor a bid price of €6, there are 76 survey responses. Out of these, 50 respondents said they would be willing to pay €6, while 26 said they would not.\nAnd so on for higher bid amounts.\n\nAs the bid price increases, we expect rationally (from law of demand) there will be a decline in the proportion of “yes” responses, since fewer people are likely to be willing to pay higher amounts for the same environmental improvement.\n\nThis pattern is visible in the table, particularly when reading across the row representing “yes” responses (often coded as 1), where the number of affirmative answers drops from 50 to 36, and continues decreasing with higher bids. Lets look at this in a graph.\n\n\n\nTo create our demand line, we will first have to convert the data in the right shape form.\n\n\nShow code\nyes_data &lt;- as.data.frame(table(NaturalPark$ans1, NaturalPark$bid1)) %&gt;%\n  rename(Answer = Var1, Bid = Var2, Count = Freq) %&gt;%\n  filter(Answer == 1) %&gt;%\n  mutate(Bid = as.numeric(as.character(Bid)))\n\n# Display the first three rows of data          \nhead(yes_data, n = 5)  \n\n\n  Answer Bid Count\n1      1   6    50\n2      1  12    43\n3      1  24    42\n4      1  48    36\n\n\nNow we will use ggplot and visually show how people respond to bid prices:\n\n\nShow code\nggplot(yes_data, aes(x = Count, y = Bid)) +\n  geom_line(color = \"darkgreen\", size = 1.2) +\n  geom_point(color = \"darkgreen\", size = 3) +\n  scale_y_continuous(breaks = yes_data$Bid) +\n  labs(\n    title = \"Decline in 'Yes' Responses with Higher Bid Prices\",\n    x = \"Number of 'Yes' Responses\",\n    y = \"Bid Amount (€)\"\n  ) +\n  theme_minimal()\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nAnd voilà! There we have a downward sloping demand curve!\nPeople respond to prices which makes sense and suggest that people are responding in a way that they do in reality.\n\n\n\nNow that we have explored the data and tested peoples rational. We are going to estimate the willingness to pay for the contingent valuation question.\nFor now, we’ll ignore gender, income, and age. The model assumes that a person’s response :\n\n0 (“no, not willing to pay”)\n1 (“yes, willing to pay”) depends only on the bid amount (or “price”).\n\nSo this model includes just one explanatory variable which is the bid.\nUsing DCchoice the formula for this type of model is as follows:\nmodel1 &lt;- sbchoice(dep var ~ exp var | bid, dist = \"\", data = my.data)\nHere’s what each part means:\n\nmodel1: The name we give to save the model results (for example, sb1 for the first model).\n&lt;-: The assignment symbol that stores the model in model1.\nsbchoice: The function that runs the model.\ndep_var: The dependent variable (the responses coded as 0 = no, 1 = yes). For our first model this is ans1.\nexp_var: The explanatory variables other than the bid amount. Since we don’t have any for the first model, we just use 1 for the intercept.\n| bid: This part specifies the bid amount variable, which is bid1 in our example.\ndist = \"\": Specifies the distribution used; we’ll always use logistic, so it will be dist = \"logistic\"\ndata = my.data: The data frame that contains the data which in our case is NaturalPark.\n\nAfter fitting the model, we usually check the results with the summary() function. To focus on the coefficient estimates, we can use coeftest() from the lmtest package.\n\n\nShow code\n# A very simple model\nsb1 &lt;- sbchoice(ans1 ~ 1 | bid1,  dist = \"logistic\", data = NaturalPark)\ncoeftest(sb1)\n\n\n\nz test of coefficients:\n\n              Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)  0.5500450  0.1998739  2.7520 0.005924 **\nBID         -0.0157223  0.0071807 -2.1895 0.028560 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe most important thing to notice in the model’s coefficients is the negative sign on the bid variable. No matter how the original data is coded, the bid will always show up as BID in the results. This negative sign means that as the bid price goes up, fewer people say “yes” they are less willing to pay that amount!\nThe results also show that this relationship is statistically significant, meaning it’s unlikely to be due to chance.\nLets see these results visually by using the plot() function. This plot shows the willingness-to-pay data along with a horizontal line marking the point where 50% of people say “yes.” From the plot, you can see that this 50% support happens at about €35.\n\n\nShow code\n# plots the predicted support at each bid value \nplot(sb1, las = 1)  \n\n# adds a horizontal line to the plot \nabline(h = 0.5, lty = 2, col = \"red\") \n\n\n\n\n\n\n\n\n\n\n\n\nWe now look at the full model summary output we get when we use the sbchoice() function:\n\n\nShow code\n#  Model summary\nsummary(sb1) \n\n\n\nCall:\nsbchoice(formula = ans1 ~ 1 | bid1, data = NaturalPark, dist = \"logistic\")\n\nFormula:\nans1 ~ 1 | bid1\n\nCoefficients: \n             Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)  0.550045   0.199874   2.752  0.00592 **\nBID         -0.015722   0.007181  -2.190  0.02856 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nDistribution: logistic \nNumber of Obs.: 312  \nlog-likelihood: -212.3968 \npseudo-R^2: 0.0113 , adjusted pseudo-R^2: 0.0020 \nLR statistic: 4.841 on 1 DF, p-value: 0.028 \nAIC: 428.793653 , BIC: 436.279659 \n\nIterations: 4  \nConvergence: TRUE \n\nWTP estimates:\n Mean : 63.95526 \n Mean : 26.04338 (truncated at the maximum bid)\n Mean : 47.26755 (truncated at the maximum bid with adjustment)\n Median : 34.98512 \n\n\nThe summary output has several parts:\n\nThe model we estimated:\nFormula:\nans1 ~ 1 | bid1\n\nCoefficients: \n             Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)  0.550045   0.199874   2.752  0.00592 **\nBID         -0.015722   0.007181  -2.190  0.02856 * \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\nThe second part shows the model’s coefficients and whether they are statistically significant.\nDistribution: logistic \nNumber of Obs.: 312  \nlog-likelihood: -212.3968 \npseudo-R^2: 0.0113 , adjusted pseudo-R^2: 0.0020 \nLR statistic: 4.841 on 1 DF, p-value: 0.028 \nAIC: 428.793653 , BIC: 436.279659 \nThe third part shows how well the model fits the data. This helps us compare different models. It’s important to see “Convergence: TRUE,” which means the model worked properly. If not, you’ll get an error.\nIterations: 4  \nConvergence: TRUE \nThe fourth part shows the estimated average and median willingness to pay (WTP). Lets take a moment here\nWTP estimates:\n Mean : 63.95526 \n Mean : 26.04338 (truncated at the maximum bid)\n Mean : 47.26755 (truncated at the maximum bid with adjustment)\n Median : 34.98512 \n\n\n\nValue: Value: 63.95\nThis estimate can be calculated by taking the coefficient of the estimation on bid:\n\\[\n1/\\beta_{bid} = 1/-0.015722=63.95\n\\]\nThis is the unbounded mean and its calculated using the full range of the estimated distribution of WTP, even beyond your maximum bid. The adjustment for the truncated mean WTP is implemented by the method of (Boyle, Welsh, and Bishop 1988)\nHow it’s calculated:\n\nUses the model’s estimated coefficients to integrate the WTP distribution from 0 to infinity (or theoretically negative infinity to positive infinity depending on the model).\nIssue: May overestimate WTP, especially if the upper tail is long (which it often is).\n\n\n\nShow code\nmedianWTP=1/sb1$coefficients[2]\nmedianWTP\n\n\n     BID \n-63.6041 \n\n\n\n\n\nsValue: 26.04\n\nThis is the mean WTP truncated at the maximum bid in your data.\n\\[\n1/\\beta_{bid}(1+exp(\\beta_0+\\sum(\\beta_nZ_n)))\n\\]\nHow it’s calculated:\n\nIntegration is limited from 0 up to the highest bid amount offered in your survey.\nWhy: This avoids overestimating WTP from extrapolating too far beyond your data.\n\n\n\nShow code\nmedianWTP * (log(1 + exp(sb1$coefficients[1] + sb1$coefficients[2] * max(NaturalPark$bid1))) - log(1 + exp(sb1$coefficients[1])))\n\n\n     BID \n26.04338 \n\n\n\n\n\n#Value: 47.27\n\nThis is the same as #2 but with a correction applied to adjust for truncation bias.\nHow it’s calculated:\n\nStill truncated at the max bid, but includes a bias-correction factor based on the distribution.\nWhy: Balances realism (truncation) with statistical accuracy (adjustment).\n\n\n\n\nValue: 34.99\nThis is the bid level at which 50% of respondents are predicted to say “yes”.\nHow it’s calculated:\n\\[\n-(\\alpha/\\beta_{bid})\n\\]\n\nIt’s the bid where the predicted probability of saying “yes” is 0.5, based on your model.\nWhy: Commonly used because it’s less sensitive to skewed data than the mean.\n\n\n\nShow code\n-sb1$coefficients[1]/sb1$coefficients[2]\n\n\n(Intercept) \n   34.98512"
  },
  {
    "objectID": "cv.html#packages",
    "href": "cv.html#packages",
    "title": "Contingent Valuation",
    "section": "",
    "text": "Open R. You will need additional packages\n\nDCchoice (Nakatani and Aizaki, n.d.)Functions for analyzing dichotomous choice contingent valuation (CV) data. It provides functions for estimating parametric and nonparametric models for single-, one-and-one-half-, and double-bounded CV data. For details, see (Aizaki et al. 2022)\nEcdat (Croissant and Graves, n.d.) For the dataset NationalPark: Willingness to Pay for the Preservation of the Alentejo Natural Park\nlmtest (Hothorn et al., n.d.)\n\n\n\nShow code\n#install.packages(\"DCchoice\",repos = c(\"http://www.bioconductor.org/packages/release/bioc\",\"https://cran.rstudio.com/\"), dep = TRUE)\n\n#install.packages(ggplot2)\n#install.packages(dplyr)\n\n\nAfter installing the necessary packages, the next step is to load DCchoice, Ecdat, and lmtest into your current R session. The DCchoice package provides the core functions used in our analysis. Additionally, lmtest offers essential tools for model testing. The Ecdat package includes publicly available real-world datasets.\nFor this example, we’ll use the NaturalParks dataset to demonstrate how contingent valuation (CV) study data can be analyzed.\n\n\nShow code\nlibrary(DCchoice)\nlibrary(Ecdat)\nlibrary(lmtest)\nlibrary(ggplot2)\nlibrary(dplyr)"
  },
  {
    "objectID": "cv.html#load-data",
    "href": "cv.html#load-data",
    "title": "Contingent Valuation",
    "section": "",
    "text": "We will use a built-in sample dataset from the Ecdat package. After loading the dataset, you can use the head() function to preview its contents. By default, head() displays the first six rows along with the column names, but you can adjust the number of rows shown by using the n argument. This function is a quick way to get an overview of the dataset’s structure.\n\n\nShow code\n# Load the data from Ecdat package\ndata(NaturalPark, package = \"Ecdat\") \n\n# Display the first three rows of data          \nhead(NaturalPark, n = 5)                     \n\n\n  bid1 bidh bidl answers age    sex income\n1    6   18    3      yy   1 female      2\n2   48  120   24      yn   2   male      1\n3   48  120   24      yn   2 female      3\n4   24   48   12      nn   5 female      1\n5   24   48   12      ny   6 female      2"
  },
  {
    "objectID": "cv.html#data-variables",
    "href": "cv.html#data-variables",
    "title": "Contingent Valuation",
    "section": "",
    "text": "The NaturalParks (NP) dataset contains seven variables, each representing a specific aspect of the survey responses:\n\nbid1: The initial bid amount (in euros) presented in the first willingness-to-pay (WTP) question. In this dataset, there are four possible bid amounts: 6, 12, 24, and 48.\nbidh: The higher follow-up bid amount, shown only if the respondent answered “yes” to the initial WTP question.\nbidl: The lower follow-up bid amount, shown only if the respondent answered “no” to the initial WTP question.\nanswers: A factor variable representing the respondent’s answers to the two WTP questions. The four possible combinations are:\n\nnn: no to both\nny: no to first, yes to second\nyn: yes to first, no to second\nyy: yes to both\n\nSince there are four combinations, this factor variable has four levels.\n\nage: Age is grouped into six brackets rather than exact values. Higher bracket numbers correspond to older respondents.\nsex: A factor variable with two levels: “male” and “female.”\nincome: Income is also grouped into eight brackets. As with age, higher numbers represent higher income levels."
  },
  {
    "objectID": "cv.html#data-manipulation",
    "href": "cv.html#data-manipulation",
    "title": "Contingent Valuation",
    "section": "",
    "text": "The decision identifies what each individual prefers. This is indicated by the “yes” and “no.” The variable needs to be converted into a numerical representation. To do this you will need to write a loop and create a binary indicator for when the individuals says yes to the first bid. This is indicated by the first letter of the variable starting with “y”\n\n\nShow code\nNaturalPark$ans1 &lt;- ifelse(NaturalPark$answers == \"yy\" | NaturalPark$answers == \"yn\", 1, 0)\n\n# Display the first three rows of data          \nhead(NaturalPark, n = 5)   \n\n\n  bid1 bidh bidl answers age    sex income ans1\n1    6   18    3      yy   1 female      2    1\n2   48  120   24      yn   2   male      1    1\n3   48  120   24      yn   2 female      3    1\n4   24   48   12      nn   5 female      1    0\n5   24   48   12      ny   6 female      2    0\n\n\nNow we want to count the number of individuals who said yes for each price. This table shows the rows as yes or no for first bid and all the prices.\n\n\nShow code\ntable(NaturalPark$ans1, NaturalPark$bid1) \n\n\n   \n     6 12 24 48\n  0 26 34 40 41\n  1 50 43 42 36\n\n\nThe table can be interpreted as follows:\n\nFor a bid price of €6, there are 76 survey responses. Out of these, 50 respondents said they would be willing to pay €6, while 26 said they would not.\nAnd so on for higher bid amounts.\n\nAs the bid price increases, we expect rationally (from law of demand) there will be a decline in the proportion of “yes” responses, since fewer people are likely to be willing to pay higher amounts for the same environmental improvement.\n\nThis pattern is visible in the table, particularly when reading across the row representing “yes” responses (often coded as 1), where the number of affirmative answers drops from 50 to 36, and continues decreasing with higher bids. Lets look at this in a graph."
  },
  {
    "objectID": "cv.html#data-visualization",
    "href": "cv.html#data-visualization",
    "title": "Contingent Valuation",
    "section": "",
    "text": "To create our demand line, we will first have to convert the data in the right shape form.\n\n\nShow code\nyes_data &lt;- as.data.frame(table(NaturalPark$ans1, NaturalPark$bid1)) %&gt;%\n  rename(Answer = Var1, Bid = Var2, Count = Freq) %&gt;%\n  filter(Answer == 1) %&gt;%\n  mutate(Bid = as.numeric(as.character(Bid)))\n\n# Display the first three rows of data          \nhead(yes_data, n = 5)  \n\n\n  Answer Bid Count\n1      1   6    50\n2      1  12    43\n3      1  24    42\n4      1  48    36\n\n\nNow we will use ggplot and visually show how people respond to bid prices:\n\n\nShow code\nggplot(yes_data, aes(x = Count, y = Bid)) +\n  geom_line(color = \"darkgreen\", size = 1.2) +\n  geom_point(color = \"darkgreen\", size = 3) +\n  scale_y_continuous(breaks = yes_data$Bid) +\n  labs(\n    title = \"Decline in 'Yes' Responses with Higher Bid Prices\",\n    x = \"Number of 'Yes' Responses\",\n    y = \"Bid Amount (€)\"\n  ) +\n  theme_minimal()\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nAnd voilà! There we have a downward sloping demand curve!\nPeople respond to prices which makes sense and suggest that people are responding in a way that they do in reality."
  },
  {
    "objectID": "cv.html#model-estimation",
    "href": "cv.html#model-estimation",
    "title": "Contingent Valuation",
    "section": "",
    "text": "Now that we have explored the data and tested peoples rational. We are going to estimate the willingness to pay for the contingent valuation question.\nFor now, we’ll ignore gender, income, and age. The model assumes that a person’s response :\n\n0 (“no, not willing to pay”)\n1 (“yes, willing to pay”) depends only on the bid amount (or “price”).\n\nSo this model includes just one explanatory variable which is the bid.\nUsing DCchoice the formula for this type of model is as follows:\nmodel1 &lt;- sbchoice(dep var ~ exp var | bid, dist = \"\", data = my.data)\nHere’s what each part means:\n\nmodel1: The name we give to save the model results (for example, sb1 for the first model).\n&lt;-: The assignment symbol that stores the model in model1.\nsbchoice: The function that runs the model.\ndep_var: The dependent variable (the responses coded as 0 = no, 1 = yes). For our first model this is ans1.\nexp_var: The explanatory variables other than the bid amount. Since we don’t have any for the first model, we just use 1 for the intercept.\n| bid: This part specifies the bid amount variable, which is bid1 in our example.\ndist = \"\": Specifies the distribution used; we’ll always use logistic, so it will be dist = \"logistic\"\ndata = my.data: The data frame that contains the data which in our case is NaturalPark.\n\nAfter fitting the model, we usually check the results with the summary() function. To focus on the coefficient estimates, we can use coeftest() from the lmtest package.\n\n\nShow code\n# A very simple model\nsb1 &lt;- sbchoice(ans1 ~ 1 | bid1,  dist = \"logistic\", data = NaturalPark)\ncoeftest(sb1)\n\n\n\nz test of coefficients:\n\n              Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)  0.5500450  0.1998739  2.7520 0.005924 **\nBID         -0.0157223  0.0071807 -2.1895 0.028560 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe most important thing to notice in the model’s coefficients is the negative sign on the bid variable. No matter how the original data is coded, the bid will always show up as BID in the results. This negative sign means that as the bid price goes up, fewer people say “yes” they are less willing to pay that amount!\nThe results also show that this relationship is statistically significant, meaning it’s unlikely to be due to chance.\nLets see these results visually by using the plot() function. This plot shows the willingness-to-pay data along with a horizontal line marking the point where 50% of people say “yes.” From the plot, you can see that this 50% support happens at about €35.\n\n\nShow code\n# plots the predicted support at each bid value \nplot(sb1, las = 1)  \n\n# adds a horizontal line to the plot \nabline(h = 0.5, lty = 2, col = \"red\")"
  },
  {
    "objectID": "cv.html#full-estimation",
    "href": "cv.html#full-estimation",
    "title": "Contingent Valuation",
    "section": "",
    "text": "We now look at the full model summary output we get when we use the sbchoice() function:\n\n\nShow code\n#  Model summary\nsummary(sb1) \n\n\n\nCall:\nsbchoice(formula = ans1 ~ 1 | bid1, data = NaturalPark, dist = \"logistic\")\n\nFormula:\nans1 ~ 1 | bid1\n\nCoefficients: \n             Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)  0.550045   0.199874   2.752  0.00592 **\nBID         -0.015722   0.007181  -2.190  0.02856 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nDistribution: logistic \nNumber of Obs.: 312  \nlog-likelihood: -212.3968 \npseudo-R^2: 0.0113 , adjusted pseudo-R^2: 0.0020 \nLR statistic: 4.841 on 1 DF, p-value: 0.028 \nAIC: 428.793653 , BIC: 436.279659 \n\nIterations: 4  \nConvergence: TRUE \n\nWTP estimates:\n Mean : 63.95526 \n Mean : 26.04338 (truncated at the maximum bid)\n Mean : 47.26755 (truncated at the maximum bid with adjustment)\n Median : 34.98512 \n\n\nThe summary output has several parts:\n\nThe model we estimated:\nFormula:\nans1 ~ 1 | bid1\n\nCoefficients: \n             Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)  0.550045   0.199874   2.752  0.00592 **\nBID         -0.015722   0.007181  -2.190  0.02856 * \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\nThe second part shows the model’s coefficients and whether they are statistically significant.\nDistribution: logistic \nNumber of Obs.: 312  \nlog-likelihood: -212.3968 \npseudo-R^2: 0.0113 , adjusted pseudo-R^2: 0.0020 \nLR statistic: 4.841 on 1 DF, p-value: 0.028 \nAIC: 428.793653 , BIC: 436.279659 \nThe third part shows how well the model fits the data. This helps us compare different models. It’s important to see “Convergence: TRUE,” which means the model worked properly. If not, you’ll get an error.\nIterations: 4  \nConvergence: TRUE \nThe fourth part shows the estimated average and median willingness to pay (WTP). Lets take a moment here\nWTP estimates:\n Mean : 63.95526 \n Mean : 26.04338 (truncated at the maximum bid)\n Mean : 47.26755 (truncated at the maximum bid with adjustment)\n Median : 34.98512 \n\n\n\nValue: Value: 63.95\nThis estimate can be calculated by taking the coefficient of the estimation on bid:\n\\[\n1/\\beta_{bid} = 1/-0.015722=63.95\n\\]\nThis is the unbounded mean and its calculated using the full range of the estimated distribution of WTP, even beyond your maximum bid. The adjustment for the truncated mean WTP is implemented by the method of (Boyle, Welsh, and Bishop 1988)\nHow it’s calculated:\n\nUses the model’s estimated coefficients to integrate the WTP distribution from 0 to infinity (or theoretically negative infinity to positive infinity depending on the model).\nIssue: May overestimate WTP, especially if the upper tail is long (which it often is).\n\n\n\nShow code\nmedianWTP=1/sb1$coefficients[2]\nmedianWTP\n\n\n     BID \n-63.6041 \n\n\n\n\n\nsValue: 26.04\n\nThis is the mean WTP truncated at the maximum bid in your data.\n\\[\n1/\\beta_{bid}(1+exp(\\beta_0+\\sum(\\beta_nZ_n)))\n\\]\nHow it’s calculated:\n\nIntegration is limited from 0 up to the highest bid amount offered in your survey.\nWhy: This avoids overestimating WTP from extrapolating too far beyond your data.\n\n\n\nShow code\nmedianWTP * (log(1 + exp(sb1$coefficients[1] + sb1$coefficients[2] * max(NaturalPark$bid1))) - log(1 + exp(sb1$coefficients[1])))\n\n\n     BID \n26.04338 \n\n\n\n\n\n#Value: 47.27\n\nThis is the same as #2 but with a correction applied to adjust for truncation bias.\nHow it’s calculated:\n\nStill truncated at the max bid, but includes a bias-correction factor based on the distribution.\nWhy: Balances realism (truncation) with statistical accuracy (adjustment).\n\n\n\n\nValue: 34.99\nThis is the bid level at which 50% of respondents are predicted to say “yes”.\nHow it’s calculated:\n\\[\n-(\\alpha/\\beta_{bid})\n\\]\n\nIt’s the bid where the predicted probability of saying “yes” is 0.5, based on your model.\nWhy: Commonly used because it’s less sensitive to skewed data than the mean.\n\n\n\nShow code\n-sb1$coefficients[1]/sb1$coefficients[2]\n\n\n(Intercept) \n   34.98512"
  },
  {
    "objectID": "tc.html",
    "href": "tc.html",
    "title": "Travel Cost",
    "section": "",
    "text": "The Travel Cost Method (TCM) is a commonly used approach to estimate the value of recreation sites. It was first suggested by Harold Hotelling in a 1947 letter to the National Park Service and then further defined by Marion Clawson (“Methods of Measuring the Demand for and Value of Outdoor Recreation. Marion Clawson. Resources for the Future, Inc., 1145 Nineteenth Street, N.W., Washington, D.C. 1959. 36p. 50c” 1972). It provides a lower-bound estimate of how much people value a site based on the cost they incur to visit.\nThere are two main types of TCM models:\n\n\n\nFocus on one specific location.\nUse Poisson regression to estimate how many trips people take.\nFrom this, you can calculate consumer surplus the benefit people get from visiting beyond what they pay.\n\nThe cost of a trip includes actual expenses and the opportunity cost of travel time.\nThese models are best when you want to estimate the total value of a site. For example, if a park is closed due to pollution or budget cuts and you want to estimate the loss in value from that closure.\n\n\n\n\n\nFocus on multiple sites or on different features (attributes) of sites.\nUse random utility models, typically estimated with a mixed logit (random parameter logit) model.\nThese models help estimate how much people value different features, like trails, toilets, or accommodation.\nUseful for park planning, as they help managers decide which improvements provide the most value to visitors.\n\n\n\n\n\nGeneral Steps in the Travel Cost Modeling Process:\n\nDefine the site to be evaluated\nIdentify the types of recreation and specify the relevant season\nDevelop a sampling strategy for data collection\nSpecify the model, including functional form and variables\nDetermine how to handle multi-purpose trips (i.e., trips with more than one goal)\nDesign and conduct the visitor survey (get data from reservation system/ mobile data)\nMeasure trip costs, including travel expenses and time value\nEstimate the model using the collected data\nCalculate the access value by estimating consumer surplus\n\n\n\n\nConsumer Surplus represents the area under the demand curve and above the actual price (travel cost) paid to access the park. This surplus reflects the net benefit or additional value visitors receive from their park experience beyond what they pay to get there. It is a commonly used metric for evaluating net recreational benefits."
  },
  {
    "objectID": "tc.html#the-travel-cost-model-tcm",
    "href": "tc.html#the-travel-cost-model-tcm",
    "title": "Travel Cost",
    "section": "",
    "text": "General Steps in the Travel Cost Modeling Process:\n\nDefine the site to be evaluated\nIdentify the types of recreation and specify the relevant season\nDevelop a sampling strategy for data collection\nSpecify the model, including functional form and variables\nDetermine how to handle multi-purpose trips (i.e., trips with more than one goal)\nDesign and conduct the visitor survey (get data from reservation system/ mobile data)\nMeasure trip costs, including travel expenses and time value\nEstimate the model using the collected data\nCalculate the access value by estimating consumer surplus\n\n\n\n\nConsumer Surplus represents the area under the demand curve and above the actual price (travel cost) paid to access the park. This surplus reflects the net benefit or additional value visitors receive from their park experience beyond what they pay to get there. It is a commonly used metric for evaluating net recreational benefits."
  },
  {
    "objectID": "tc.html#count-model",
    "href": "tc.html#count-model",
    "title": "Travel Cost",
    "section": "Count Model",
    "text": "Count Model\nSimple cost and the decision of a trip\n\n\nShow code\nmodel1=glm(trip ~ cost, \n            data = park, \n            family = poisson())\nsummary(model1)\n\n\n\nCall:\nglm(formula = trip ~ cost, family = poisson(), data = park)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  1.255e+00  8.017e-03  156.59   &lt;2e-16 ***\ncost        -2.716e-03  4.525e-05  -60.01   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 51093  on 31584  degrees of freedom\nResidual deviance: 47034  on 31583  degrees of freedom\n  (1421 observations deleted due to missingness)\nAIC: 124676\n\nNumber of Fisher Scoring iterations: 5\n\n\n\nCalculate the access value by estimating consumer surplus\n\n\nSimple WTP\n\n\nShow code\n1/model1$coefficients[2]\n\n\n     cost \n-368.2565 \n\n\nThe interpretation suggest that on average the consumer surplus for each person who camps at this park is on average $368."
  },
  {
    "objectID": "tc.html#more-controls",
    "href": "tc.html#more-controls",
    "title": "Travel Cost",
    "section": "More Controls",
    "text": "More Controls\nMultiple variable regression: Controlling for more factors in the model\n\n\nShow code\nmodel2=glm(trip ~ cost + income+factor(year)+temp_avg+mdays+factor(sitetype), \n            data = park, \n            family = poisson())\n\nsummary(model2)\n\n\n\nCall:\nglm(formula = trip ~ cost + income + factor(year) + temp_avg + \n    mdays + factor(sitetype), family = poisson(), data = park)\n\nCoefficients:\n                                    Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                       -1.336e+00  7.652e-02 -17.459  &lt; 2e-16 ***\ncost                              -3.308e-03  4.894e-05 -67.585  &lt; 2e-16 ***\nincome                             6.063e-06  1.888e-07  32.106  &lt; 2e-16 ***\nfactor(year)2019                  -3.635e-02  1.275e-02  -2.851 0.004356 ** \nfactor(year)2020                   5.408e-02  1.502e-02   3.599 0.000319 ***\nfactor(year)2021                   5.592e-02  1.246e-02   4.489 7.17e-06 ***\nfactor(year)2022                   1.822e-02  1.292e-02   1.410 0.158455    \nfactor(year)2023                   5.927e-03  1.392e-02   0.426 0.670318    \ntemp_avg                           2.020e-02  5.505e-04  36.694  &lt; 2e-16 ***\nmdays                              3.685e-02  2.914e-03  12.646  &lt; 2e-16 ***\nfactor(sitetype)ADA Standard Full  1.646e-02  9.708e-02   0.170 0.865350    \nfactor(sitetype)ADA TENT           1.324e-01  1.047e-01   1.265 0.205737    \nfactor(sitetype)GROUP TENT ONLY   -9.854e-02  8.400e-02  -1.173 0.240766    \nfactor(sitetype)HOST SITE          2.820e-01  1.917e-01   1.471 0.141265    \nfactor(sitetype)STANDARD           8.083e-01  6.718e-02  12.032  &lt; 2e-16 ***\nfactor(sitetype)STANDARD - FULL    7.389e-01  6.729e-02  10.982  &lt; 2e-16 ***\nfactor(sitetype)TENT SITE          1.146e+00  6.706e-02  17.090  &lt; 2e-16 ***\nfactor(sitetype)YURT               7.164e-01  6.754e-02  10.608  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 51093  on 31584  degrees of freedom\nResidual deviance: 40221  on 31567  degrees of freedom\n  (1421 observations deleted due to missingness)\nAIC: 117894\n\nNumber of Fisher Scoring iterations: 5\n\n\n\nWTP\n\n\nShow code\n1/model2$coefficients[2]\n\n\n    cost \n-302.317 \n\n\nWe can see with more controls our measurement reduces and is more conservative.\nThe consumer surplus is now around $302 per person.\nFrom here you could take the sum of the season or year and calculate the WTP for this specific site. For example, a policy in which the park will impacted the park, funding for park, etc. This is helpful in estimating the total use value (consumer surplus) of one site."
  },
  {
    "objectID": "hm.html",
    "href": "hm.html",
    "title": "Revealed Preference: Hedonic Modeling",
    "section": "",
    "text": "There might be more questions on CV and DCE but for now we are switching gears and moving on the the Hedonic modeling."
  },
  {
    "objectID": "hm.html#bias",
    "href": "hm.html#bias",
    "title": "Revealed Preference: Hedonic Modeling",
    "section": "Bias",
    "text": "Bias\n\nRemoved\n\nHypothetical bias\nIn stated preference methods (like surveys or interviews), people describe what they say they would do in a certain situation. But their answers might not reflect what they’d actually do in real life.\n\nExample: A tourist might say in a survey, “I care a lot about sustainability and would pay extra for an eco-friendly tour,” but in reality, they might book the cheapest option when the time comes.\nRevealed preferences rely on actual behavior (e.g., trips they take, community lived in, choices made), so they avoid this disconnect.\n\n\n\n\nIntroduced\n\nOmitted variable\nPeople’s choices are observed, but the reasons behind them aren’t. You can’t always tell why someone made a particular choice. Maybe they chose an option because it was cheapest, or maybe because of convenience or habit. This brings up equity issues.\n\n\nConfounding factors\nExternal factors (like advertising, social influence, availability) may drive choices, not just personal preferences.\n\n\nExisting Choices\nRevealed preferences only capture behavior in the current market or context. If a product or experience doesn’t exist yet, revealed preference can’t tell you how people would respond to it.\n\n\nEndogeneity\nThe observed behavior might be influenced by the very thing you’re trying to measure (e.g., pricing strategies shaped by demand). This can complicate causal analysis."
  },
  {
    "objectID": "hm.html#libraries",
    "href": "hm.html#libraries",
    "title": "Revealed Preference: Hedonic Modeling",
    "section": "Libraries",
    "text": "Libraries\n\n\nShow code\nlibrary(wooldridge)\n\n\nWarning: package 'wooldridge' was built under R version 4.3.3\n\n\nShow code\nlibrary(lmtest)\nlibrary(ggplot2)\n\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nShow code\nlibrary(dplyr)"
  },
  {
    "objectID": "hm.html#data",
    "href": "hm.html#data",
    "title": "Revealed Preference: Hedonic Modeling",
    "section": "Data",
    "text": "Data\n\n\nShow code\n# Load the data from Ecdat package\ndata(kielmc, package = \"wooldridge\") \n# Display the first three rows of data \nhead(kielmc, n = 5)\n\n\n  year age agesq nbh  cbd intst lintst price rooms area  land baths  dist\n1 1978  48  2304   4 3000  1000 6.9078 60000     7 1660  4578     1 10700\n2 1978  83  6889   4 4000  1000 6.9078 40000     6 2612  8370     2 11000\n3 1978  58  3364   4 4000  1000 6.9078 34000     6 1144  5000     1 11500\n4 1978  11   121   4 4000  1000 6.9078 63900     5 1136 10000     1 11900\n5 1978  48  2304   4 4000  2000 7.6009 44000     5 1868 10000     1 12100\n     ldist wind   lprice y81    larea    lland y81ldist lintstsq nearinc\n1 9.277999    3 11.00210   0 7.414573 8.429017        0 47.71770       1\n2 9.305651    3 10.59663   0 7.867871 9.032409        0 47.71770       1\n3 9.350102    3 10.43412   0 7.042286 8.517193        0 47.71770       1\n4 9.384294    3 11.06507   0 7.035269 9.210340        0 47.71770       1\n5 9.400961    3 10.69195   0 7.532624 9.210340        0 57.77368       1\n  y81nrinc rprice  lrprice\n1        0  60000 11.00210\n2        0  40000 10.59663\n3        0  34000 10.43412\n4        0  63900 11.06507\n5        0  44000 10.69195\n\n\nKiel and McClain (1995)(Kiel and McClain 1995) studied how a new garbage incinerator affected house prices in North Andover, Massachusetts. They used many years of data and complex analysis. In our case, we’ll use just two years of data and simpler models, but the goal is the same.\nRumors about the incinerator started after 1978, and construction began in 1981. It was expected to start working shortly after construction began, but it actually opened in 1985. We’ll compare house prices from 1978 (before construction) and 1981 (when construction began).\nOur hypothesis is that houses close to the incinerator would drop in value compared to those farther away.\nFor this example, a house is considered “near” the incinerator if it’s within three miles. (In another exercise, you’ll use the exact distance, like Kiel and McClain did.)\nTo measure the impact, we’ll look at house prices in terms of 1978 dollars, adjusting for inflation using the Boston housing price index. We’ll call this inflation-adjusted price rprice.\n\n\n\nIncineration Waste Disposal\n\n\nA dataframe containing 321 observations on 25 variables:\n\nyear: 1978 or 1981\nage: age of house\nagesq: age^2\nnbh: neighborhood, 1-6\ncbd: dist. to cent. bus. dstrct, ft.\nintst: dist. to interstate, ft.\nlintst: log(intst)\nprice: selling price\nrooms: # rooms in house\narea: square footage of house\nland: square footage lot\nbaths: # bathrooms\ndist: dist. from house to incin., ft.\nldist: log(dist)\nwind: prc. time wind incin. to house\nlprice: log(price)\ny81: =1 if year == 1981\nlarea: log(area)\nlland: log(land)\ny81ldist: y81*ldist\nlintstsq: lintst^2\nnearinc: =1 if dist &lt;= 15840\ny81nrinc: y81*nearinc\nrprice: price, 1978 dollars\nlrprice: log(rprice)\n\n\n\nShow code\nboxplot(rprice~as.factor(rooms),las =1, col=\"grey\", outcol =\"grey\",\n        ylab = \"\", xlab = \"Number of bedroom\",  \n        main = \"Price and bedrooms\", horizontal =F,\n        data = kielmc)\n\n\n\n\n\n\n\n\n\nShow code\nboxplot(rprice~as.factor(nearinc),las =1, col=\"grey\", outcol =\"grey\",\n        ylab = \"\", xlab = \"Price\",boxwex =0.6,  \n        main = \"Price Near Incinerator\", horizontal =F,\n        data = kielmc)\n\n\n\n\n\n\n\n\n\nShow code\nboxplot(price~as.factor(y81),las =1, col=\"grey\", outcol =\"grey\",\n        ylab = \"\", xlab = \"Price\",  boxwex =0.6,\n        main = \"Price in 1978 vs 1981\", horizontal =F,\n        data = kielmc)\n\n\n\n\n\n\n\n\n\nShow code\nboxplot(price~as.factor(y81nrinc),las =1, col=\"grey\", outcol =\"grey\",\n        ylab = \"\", xlab = \"Price\",  boxwex =0.6,\n        main = \"Price and After 1981\", horizontal =F,\n        data = kielmc)\n\n\n\n\n\n\n\n\n\n\nSimple Model\nA very naive model would look at the change in price in the change in price over the two time periods\n\\[\n\\hat{rprice} =  \\beta_0 - \\beta_1nearinc+u\n\\]\nTo run this in r simply run the following equation\n\n\nShow code\nmodel0=lm(rprice~nearinc, data=subset(kielmc, y81==1 ))\nsummary(model0)\n\n\n\nCall:\nlm(formula = rprice ~ nearinc, data = subset(kielmc, y81 == 1))\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-60678 -19832  -2997  21139 136754 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   101308       3093  32.754  &lt; 2e-16 ***\nnearinc       -30688       5828  -5.266 5.14e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 31240 on 140 degrees of freedom\nMultiple R-squared:  0.1653,    Adjusted R-squared:  0.1594 \nF-statistic: 27.73 on 1 and 140 DF,  p-value: 5.139e-07\n\n\nThis binary indicator is equal to one when the year is 1987 and the house is close the the incinerator:\n\\[\n\\hat{rprice} =  101,307.5 + 30,688.27nearinc+u\n\\]\nSince this is a simple regression with just one dummy variable, the intercept represents the average house price for homes not near the incinerator. The coefficient on the variable nearinc shows how much the average price changes for homes near the incinerator.\nThe result shows that, on average, homes near the incinerator sold for $30,688.27 less than homes farther away.\nBUT it does not imply that the siting of the incinerator is causing the lower housing values. In fact if we look at the value at those houses before 1981 (like we see in the boxplots above) we obtain this\n\n\nShow code\nmodel1=lm(rprice~nearinc, data=subset(kielmc, y81==0 ))\nsummary(model1)\n\n\n\nCall:\nlm(formula = rprice ~ nearinc, data = subset(kielmc, y81 == 0))\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-56517 -16605  -3193   8683 236307 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    82517       2654  31.094  &lt; 2e-16 ***\nnearinc       -18824       4745  -3.968 0.000105 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 29430 on 177 degrees of freedom\nMultiple R-squared:  0.08167,   Adjusted R-squared:  0.07648 \nF-statistic: 15.74 on 1 and 177 DF,  p-value: 0.0001054\n\n\nWe can see that those close to the incinerator were less than the average value of a home not near the site. This comes back to the issue mentioned above in the biases in data. The area of where the incinerator was built appears to be in a poorer neighborhood based on housing prices.\n** Note we will talk about equity issues in the environmental justice section. **\n\n\nDifference-in-Difference\nDifference-in-Difference modeling (DiffNDiff for short). Is often how economist will try to control for the issue presented above.\nFirst we will both control for the estimation for being near the incinerator (nearinc), the year the data is collected (y81) and the difference of the price for houses near construction of the incinerator began construction in 1981 (y81nrinc).\n\n\nShow code\nmodel2=lm(rprice~y81+nearinc+y81nrinc, data=kielmc)\nsummary(model2)\n\n\n\nCall:\nlm(formula = rprice ~ y81 + nearinc + y81nrinc, data = kielmc)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-60678 -17693  -3031  12483 236307 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    82517       2727  30.260  &lt; 2e-16 ***\ny81            18790       4050   4.640 5.12e-06 ***\nnearinc       -18824       4875  -3.861 0.000137 ***\ny81nrinc      -11864       7457  -1.591 0.112595    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 30240 on 317 degrees of freedom\nMultiple R-squared:  0.1739,    Adjusted R-squared:  0.1661 \nF-statistic: 22.25 on 3 and 317 DF,  p-value: 4.224e-13\n\n\nFrom here we can see houses located near the incinerator were valued less in 1978 (-$18.8K). After construction in 1981, the variable estimated is not significant.\n\n\nOmitted Variables\nThe models we just ran are missing a lot of information. Kiel and McClain (1995)(Kiel and McClain 1995) included housing features in their study for two main reasons:\n\nThe types of homes sold near the incinerator in 1981 might have been different from those sold in 1978. If so, it’s important to account for those differences.\nEven if the homes didn’t change, including their characteristics helps reduce the amount of unexplained variation (error), which in turn makes the estimates more precise.\n\nIn column (3) of the results, they include the age of the houses (in a quadratic form). This improves the model by increasing the R-squared and reducing error. As a result, the effect of being near the incinerator in 1981 (y81·nearinc) becomes larger in size and more precisely estimated (lower standard error).\nIn column (4) we additionally control for distance to the interstate in feet (intst), land area in feet (land), house area in feet (area), number of rooms (rooms), and number of baths (baths).\nLastly, column (5) uses lrprice: log(rprice) so that we can interpret the entire equation as percentage changes in order to get the effect and address adjusting for inflation.\n\n\nShow code\nlibrary(stargazer)\n\n\nWarning: package 'stargazer' was built under R version 4.3.3\n\n\n\nPlease cite as: \n\n\n Hlavac, Marek (2022). stargazer: Well-Formatted Regression and Summary Statistics Tables.\n\n\n R package version 5.2.3. https://CRAN.R-project.org/package=stargazer \n\n\nShow code\n# Run the models\nmodel3 &lt;- lm(rprice ~ y81 + nearinc + y81nrinc + age + agesq, data = kielmc)\nmodel4 &lt;- lm(rprice ~ y81 + nearinc + y81nrinc + age + agesq + intst + land + area + rooms + baths, data = kielmc)\n\nmodel5 &lt;- lm(lrprice ~ y81 + nearinc + y81nrinc + age + agesq + intst + land + area + rooms + baths, data = kielmc)\n\n# Create the table\nstargazer(model0,model1, model2, model3, model4,model5,type = \"text\", title = \"Regression Results\", column.labels = c(\"Simple81\",\"Simple78\", \"DiffNDiff\", \"Age\", \"Full\", \"LogPrice\"))\n\n\n\nRegression Results\n=====================================================================================================================================================================\n                                                                                   Dependent variable:                                                               \n                    -------------------------------------------------------------------------------------------------------------------------------------------------\n                                                                             rprice                                                                  lrprice        \n                           Simple81                Simple78                DiffNDiff                  Age                     Full                   LogPrice        \n                              (1)                     (2)                     (3)                     (4)                     (5)                      (6)           \n---------------------------------------------------------------------------------------------------------------------------------------------------------------------\ny81                                                                      18,790.290***           21,321.040***           13,928.480***               0.139***        \n                                                                          (4,050.065)             (3,443.631)             (2,798.747)                (0.029)         \n                                                                                                                                                                     \nnearinc                 -30,688.270***          -18,824.370***          -18,824.370***            9,397.936*               3,780.337                  -0.035         \n                          (5,827.709)             (4,744.594)             (4,875.322)             (4,812.222)             (4,453.415)                (0.046)         \n                                                                                                                                                                     \ny81nrinc                                                                  -11,863.900           -21,920.270***           -14,177.930***              -0.093*         \n                                                                          (7,456.646)             (6,359.745)             (4,987.267)                (0.052)         \n                                                                                                                                                                     \nage                                                                                              -1,494.424***            -739.451***               -0.009***        \n                                                                                                   (131.860)               (131.127)                 (0.001)         \n                                                                                                                                                                     \nagesq                                                                                              8.691***                 3.453***                0.00004***       \n                                                                                                    (0.848)                 (0.813)                 (0.00001)        \n                                                                                                                                                                     \nintst                                                                                                                      -0.539***                -0.00000*        \n                                                                                                                            (0.196)                 (0.00000)        \n                                                                                                                                                                     \nland                                                                                                                        0.141***                0.00000***       \n                                                                                                                            (0.031)                 (0.00000)        \n                                                                                                                                                                     \narea                                                                                                                       18.086***                0.0002***        \n                                                                                                                            (2.306)                 (0.00002)        \n                                                                                                                                                                     \nrooms                                                                                                                     3,304.227**                0.053***        \n                                                                                                                          (1,661.248)                (0.017)         \n                                                                                                                                                                     \nbaths                                                                                                                     6,977.317***               0.103***        \n                                                                                                                          (2,581.321)                (0.027)         \n                                                                                                                                                                     \nConstant                101,307.500***           82,517.230***           82,517.230***           89,116.540***             13,807.670               10.371***        \n                          (3,093.027)             (2,653.790)             (2,726.910)             (2,406.051)             (11,166.590)               (0.117)         \n                                                                                                                                                                     \n---------------------------------------------------------------------------------------------------------------------------------------------------------------------\nObservations                  142                     179                     321                     321                     321                      321           \nR2                           0.165                   0.082                   0.174                   0.414                   0.660                    0.730          \nAdjusted R2                  0.159                   0.076                   0.166                   0.405                   0.649                    0.721          \nResidual Std. Error  31,238.040 (df = 140)   29,431.960 (df = 177)   30,242.900 (df = 317)   25,543.290 (df = 315)   19,619.020 (df = 310)       0.205 (df = 310)    \nF Statistic         27.730*** (df = 1; 140) 15.741*** (df = 1; 177) 22.251*** (df = 3; 317) 44.591*** (df = 5; 315) 60.189*** (df = 10; 310) 83.880*** (df = 10; 310)\n=====================================================================================================================================================================\nNote:                                                                                                                                     *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\nWhat we can see in the above regressions, at first we see in columns 1-3 the location of the house to the incinerator is highly significant. The incinerator construction location was built in a poorer neighborhood.\nOnce we control for the housing characteristics (columns 4-6), we find that the construction of the incinerator decreased housing prices by 14 to 21K or more intuitively housing prices dropped by 9.3%.\nThis all suggest that undesirable facilities known to impact many aspects of the quality of environment greatly impacts the overall assets of the people chosen to bear the change!!\n\n\n\nTrash incinerators disproportionately harm Black and Hispanic people - TheGrio"
  },
  {
    "objectID": "rp.html",
    "href": "rp.html",
    "title": "Revealed Preference",
    "section": "",
    "text": "Stated Preferences\n\n\n\nIts So Exhausting Waiting For Death GIFs - Find & Share on GIPHY"
  },
  {
    "objectID": "rp.html#we-will-be-using-this-after",
    "href": "rp.html#we-will-be-using-this-after",
    "title": "Revealed Preference",
    "section": "",
    "text": "Stated Preferences\n\n\n\nIts So Exhausting Waiting For Death GIFs - Find & Share on GIPHY"
  },
  {
    "objectID": "sp.html",
    "href": "sp.html",
    "title": "Stated Preference",
    "section": "",
    "text": "Core Stated Preference Methods:\n\nContingent Valuation\nThis method directly asks individuals about their willingness to pay for a good or service, or their willingness to accept compensation for its loss. In R, packages can be used to analyze responses from single-bounded or double-bounded dichotomous choice CV surveys, employing both parametric and nonparametric approaches.\nDiscrete Choice\nDCEs present respondents with several choice scenarios, each containing multiple alternatives described by various attributes and their levels. Respondents choose their preferred alternative in each scenario. R packages facilitate the design of DCEs (e.g., using orthogonal main-effect designs) and the analysis of choice data using models like conditional and binary logit."
  },
  {
    "objectID": "sp.html#example",
    "href": "sp.html#example",
    "title": "Stated Preference",
    "section": "Example",
    "text": "Example\nBased on James Fogarty and Hideo Aizaki\n\nPackages\nOpen R. You will need additional packages DCchoice, Ecdat (Croissant and Graves 2020) and lmtest (Zeileis and Hothorn 2002).\n\n#install.packages(\"DCchoice\",repos = c(\"http://www.bioconductor.org/packages/release/bioc\",\"https://cran.rstudio.com/\"), dep = TRUE)\n\n#install.packages(ggplot2)\n#install.packages(dplyr)\n\nAfter installing the necessary packages, the next step is to load DCchoice, Ecdat, and lmtest into your current R session. The DCchoice package provides the core functions used in our analysis. Additionally, lmtest offers essential tools for model testing. The Ecdat package includes publicly available real-world datasets.\nFor this example, we’ll use the NaturalParks dataset to demonstrate how contingent valuation (CV) study data can be analyzed.\n\nlibrary(DCchoice)\nlibrary(Ecdat)\nlibrary(lmtest)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\n\nLoad Data\nWe will use a built-in sample dataset from the Ecdat package. After loading the dataset, you can use the head() function to preview its contents. By default, head() displays the first six rows along with the column names, but you can adjust the number of rows shown by using the n argument. This function is a quick way to get an overview of the dataset’s structure.\n\n# Load the data from Ecdat package\ndata(NaturalPark, package = \"Ecdat\") \n\n# Display the first three rows of data          \nhead(NaturalPark, n = 5)                     \n\n  bid1 bidh bidl answers age    sex income\n1    6   18    3      yy   1 female      2\n2   48  120   24      yn   2   male      1\n3   48  120   24      yn   2 female      3\n4   24   48   12      nn   5 female      1\n5   24   48   12      ny   6 female      2\n\n\n\n\nData Variables\nThe NaturalParks (NP) dataset contains seven variables, each representing a specific aspect of the survey responses:\n\nbid1: The initial bid amount (in euros) presented in the first willingness-to-pay (WTP) question. In this dataset, there are four possible bid amounts: 6, 12, 24, and 48.\nbidh: The higher follow-up bid amount, shown only if the respondent answered “yes” to the initial WTP question.\nbidl: The lower follow-up bid amount, shown only if the respondent answered “no” to the initial WTP question.\nanswers: A factor variable representing the respondent’s answers to the two WTP questions. The four possible combinations are:\n\nnn: no to both\nny: no to first, yes to second\nyn: yes to first, no to second\nyy: yes to both\n\nSince there are four combinations, this factor variable has four levels.\n\nage: Age is grouped into six brackets rather than exact values. Higher bracket numbers correspond to older respondents.\nsex: A factor variable with two levels: “male” and “female.”\nincome: Income is also grouped into eight brackets. As with age, higher numbers represent higher income levels.\n\n\n\nData Manipulation\nThe decision identifies what each individual prefers. This is indicated by the “yes” and “no.” The variable needs to be converted into a numerical representation. To do this you will need to write a loop and create a binary indicator for when the individuals says yes to the first bid. This is indicated by the first letter of the variable starting with “y”\n\nNaturalPark$ans1 &lt;- ifelse(NaturalPark$answers == \"yy\" | NaturalPark$answers == \"yn\", 1, 0)\n\n# Display the first three rows of data          \nhead(NaturalPark, n = 5)   \n\n  bid1 bidh bidl answers age    sex income ans1\n1    6   18    3      yy   1 female      2    1\n2   48  120   24      yn   2   male      1    1\n3   48  120   24      yn   2 female      3    1\n4   24   48   12      nn   5 female      1    0\n5   24   48   12      ny   6 female      2    0\n\n\nNow we want to count the number of individuals who said yes for each price. This table shows the rows as yes or no for first bid and all the prices.\n\ntable(NaturalPark$ans1, NaturalPark$bid1) \n\n   \n     6 12 24 48\n  0 26 34 40 41\n  1 50 43 42 36\n\n\nThe table can be interpreted as follows:\n\nFor a bid price of €6, there are 76 survey responses. Out of these, 50 respondents said they would be willing to pay €6, while 26 said they would not.\nAnd so on for higher bid amounts.\n\nAs the bid price increases, we expect rationally (from law of demand) there will be a decline in the proportion of “yes” responses, since fewer people are likely to be willing to pay higher amounts for the same environmental improvement.\n\nThis pattern is visible in the table, particularly when reading across the row representing “yes” responses (often coded as 1), where the number of affirmative answers drops from 50 to 36, and continues decreasing with higher bids. Lets look at this in a graph.\n\n\nData Visualization\nTo create our demand line, we will first have to convert the data in the right shape form.\n\nyes_data &lt;- as.data.frame(table(NaturalPark$ans1, NaturalPark$bid1)) %&gt;%\n  rename(Answer = Var1, Bid = Var2, Count = Freq) %&gt;%\n  filter(Answer == 1) %&gt;%\n  mutate(Bid = as.numeric(as.character(Bid)))\n\n# Display the first three rows of data          \nhead(yes_data, n = 5)  \n\n  Answer Bid Count\n1      1   6    50\n2      1  12    43\n3      1  24    42\n4      1  48    36\n\n\nNow we will use ggplot and visually show how people respond to bid prices:\n\nggplot(yes_data, aes(x = Count, y = Bid)) +\n  geom_line(color = \"darkgreen\", size = 1.2) +\n  geom_point(color = \"darkgreen\", size = 3) +\n  scale_y_continuous(breaks = yes_data$Bid) +\n  labs(\n    title = \"Decline in 'Yes' Responses with Higher Bid Prices\",\n    x = \"Number of 'Yes' Responses\",\n    y = \"Bid Amount (€)\"\n  ) +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nAnd voilà! There we have a downward sloping demand curve!\nPeople respond to prices which makes sense and suggest that people are responding in a way that they do in reality.\n\n\nModel Estimation\nNow that we have explored the data and tested peoples rational. We are going to estimate the willingness to pay for the contingent valuation question.\nFor now, we’ll ignore gender, income, and age. The model assumes that a person’s response :\n\n0 (“no, not willing to pay”)\n1 (“yes, willing to pay”) depends only on the bid amount (or “price”).\n\nSo this model includes just one explanatory variable which is the bid.\nUsing DCchoice the formula for this type of model is as follows:\nmodel1 &lt;- sbchoice(dep var ~ exp var | bid, dist = \"\", data = my.data)\nHere’s what each part means:\n\nmodel1: The name we give to save the model results (for example, sb1 for the first model).\n&lt;-: The assignment symbol that stores the model in model1.\nsbchoice: The function that runs the model.\ndep_var: The dependent variable (the responses coded as 0 = no, 1 = yes). For our first model this is ans1.\nexp_var: The explanatory variables other than the bid amount. Since we don’t have any for the first model, we just use 1 for the intercept.\n| bid: This part specifies the bid amount variable, which is bid1 in our example.\ndist = \"\": Specifies the distribution used; we’ll always use logistic, so it will be dist = \"logistic\"\ndata = my.data: The data frame that contains the data which in our case is NaturalPark.\n\nAfter fitting the model, we usually check the results with the summary() function. To focus on the coefficient estimates, we can use coeftest() from the lmtest package.\n\n# A very simple model\nsb1 &lt;- sbchoice(ans1 ~ 1 | bid1,  dist = \"logistic\", data = NaturalPark)\ncoeftest(sb1)\n\n\nz test of coefficients:\n\n              Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)  0.5500450  0.1998739  2.7520 0.005924 **\nBID         -0.0157223  0.0071807 -2.1895 0.028560 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe most important thing to notice in the model’s coefficients is the negative sign on the bid variable. No matter how the original data is coded, the bid will always show up as BID in the results. This negative sign means that as the bid price goes up, fewer people say “yes” they are less willing to pay that amount!\nThe results also show that this relationship is statistically significant, meaning it’s unlikely to be due to chance.\nLets see these results visually by using the plot() function. This plot shows the willingness-to-pay data along with a horizontal line marking the point where 50% of people say “yes.” From the plot, you can see that this 50% support happens at about €35.\n\n# plots the predicted support at each bid value \nplot(sb1, las = 1)  \n\n# adds a horizontal line to the plot \nabline(h = 0.5, lty = 2, col = \"red\") \n\n\n\n\n\n\n\n\n\n\nFull Estimation\nWe now look at the full model summary output we get when we use the sbchoice() function:\n\n#  Model summary\nsummary(sb1) \n\n\nCall:\nsbchoice(formula = ans1 ~ 1 | bid1, data = NaturalPark, dist = \"logistic\")\n\nFormula:\nans1 ~ 1 | bid1\n\nCoefficients: \n             Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)  0.550045   0.199874   2.752  0.00592 **\nBID         -0.015722   0.007181  -2.190  0.02856 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nDistribution: logistic \nNumber of Obs.: 312  \nlog-likelihood: -212.3968 \npseudo-R^2: 0.0113 , adjusted pseudo-R^2: 0.0020 \nLR statistic: 4.841 on 1 DF, p-value: 0.028 \nAIC: 428.793653 , BIC: 436.279659 \n\nIterations: 4  \nConvergence: TRUE \n\nWTP estimates:\n Mean : 63.95526 \n Mean : 26.04338 (truncated at the maximum bid)\n Mean : 47.26755 (truncated at the maximum bid with adjustment)\n Median : 34.98512 \n\n\nThe summary output has several parts:\n\nThe model we estimated:\nFormula:\nans1 ~ 1 | bid1\n\nCoefficients: \n             Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)  0.550045   0.199874   2.752  0.00592 **\nBID         -0.015722   0.007181  -2.190  0.02856 * \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\nThe second part shows the model’s coefficients and whether they are statistically significant.\nDistribution: logistic \nNumber of Obs.: 312  \nlog-likelihood: -212.3968 \npseudo-R^2: 0.0113 , adjusted pseudo-R^2: 0.0020 \nLR statistic: 4.841 on 1 DF, p-value: 0.028 \nAIC: 428.793653 , BIC: 436.279659 \nThe third part shows how well the model fits the data. This helps us compare different models. It’s important to see “Convergence: TRUE,” which means the model worked properly. If not, you’ll get an error.\nIterations: 4  \nConvergence: TRUE \nThe fourth part shows the estimated average and median willingness to pay (WTP). Lets take a moment here\nWTP estimates:\n Mean : 63.95526 \n Mean : 26.04338 (truncated at the maximum bid)\n Mean : 47.26755 (truncated at the maximum bid with adjustment)\n Median : 34.98512 \n\n\n1. Mean WTP\nValue: Value: 63.95\nThis estimate can be calculated by taking the coefficient of the estimation on bid:\n\\(1/\\beta_{bid} = 1/-0.015722=63.95\\)\nThis is the unbounded mean and its calculated using the full range of the estimated distribution of WTP, even beyond your maximum bid.\nHow it’s calculated:\n\nUses the model’s estimated coefficients to integrate the WTP distribution from 0 to infinity (or theoretically negative infinity to positive infinity depending on the model).\nIssue: May overestimate WTP, especially if the upper tail is long (which it often is).\n\n\nmedianWTP=1/sb1$coefficients[2]\nmedianWTP\n\n     BID \n-63.6041 \n\n\n\n\n2. Mean WTP (truncated at max bid)\nValue: 26.04\n\nThis is the mean WTP truncated at the maximum bid in your data.\n\\(1/\\beta_{bid}(1+exp(\\beta_0+\\sum(\\beta_nZ_n)))\\)\nHow it’s calculated:\n\nIntegration is limited from 0 up to the highest bid amount offered in your survey.\nWhy: This avoids overestimating WTP from extrapolating too far beyond your data.\n\n\nmedianWTP * (log(1 + exp(sb1$coefficients[1] + sb1$coefficients[2] * max(NaturalPark$bid1))) - log(1 + exp(sb1$coefficients[1])))\n\n     BID \n26.04338 \n\n\n\n\n3. Mean WTP (truncated with adjustment)\nValue: 47.27\n\nThis is the same as #2 but with a correction applied to adjust for truncation bias.\nHow it’s calculated:\n\nStill truncated at the max bid, but includes a bias-correction factor based on the distribution. the truncated mean WTP with the adjustment of Boyle et~al.(1988)\nWhy: Balances realism (truncation) with statistical accuracy (adjustment).\n\n\n\n4. Median WTP\nValue: 34.99\n\nThis is the bid level at which 50% of respondents are predicted to say “yes”.\nHow it’s calculated:\n\\(-(\\alpha/\\beta_{bid})\\)\n\nIt’s the bid where the predicted probability of saying “yes” is 0.5, based on your model.\nWhy: Commonly used because it’s less sensitive to skewed data than the mean.\n\n\n-sb1$coefficients[1]/sb1$coefficients[2]\n\n(Intercept) \n   34.98512"
  },
  {
    "objectID": "sp.html#attribute-levels",
    "href": "sp.html#attribute-levels",
    "title": "Stated Preference",
    "section": "Attribute & Levels",
    "text": "Attribute & Levels\n\n\n\n\n\n\n\n\nAttributes\nLabels\nLevels\n\n\n\n\nSize of Wind Farm (discrete)\nSmall Farms\n0\n\n\nNote reference is LargeFarm\n\n1\n\n\n\nMediumFarms\n0\n\n\n\n\n1\n\n\nMax. Height Turbine (discrete)\nLow Height\n0\n\n\nNote reference is HighHeight\n\n1\n\n\n\nMedium Height\n0\n\n\n\n\n1\n\n\nReduction in Red Kite (continous)\nRed Kite\n5\n\n\n\n\n7.5\n\n\n\n\n10\n\n\n\n\n12.5\n\n\n\n\n15\n\n\nDistance to residents (continous)\nMinDistance\n750\n\n\n\n\n1000\n\n\n\n\n1250\n\n\n\n\n1500\n\n\n\n\n1750\n\n\nMonthlyCost\n(Continous)\nCost\n0\n\n\n\n\n1\n\n\n\n\n2\n\n\n\n\n3\n\n\n\n\n4\n\n\n\n\n…..\n\n\n\n\n….\n\n\n\n\n10"
  },
  {
    "objectID": "sp.html#choice-set",
    "href": "sp.html#choice-set",
    "title": "Stated Preference",
    "section": "Choice Set",
    "text": "Choice Set\nLets first consider this example\n\nThis choice card has 3 alternatives and thus 3 different utility functions you are estimating:\n\nThe status quo or the opting out and keeping things the way they are. The utility function would look something like this:\n\n\\[\nU_{n1t}=\n\\beta_{mf}Med.Farms_{n1t}+\\beta_{sf}SmallFarms_{n1t} \\\\\n+\\beta_{mh}Med.Height_{n1t}+\\beta_{lh}LowHeight_{n1t} \\\\\n+\\beta_{rk}redKite_{n|t}+\\beta_{md}MinDistance_{n1t} \\\\\n\\beta_{cost}Cost_{n1t}+ \\epsilon_{n1t}\n\\]\n\nProgram B will be alternative 2 and thus is indexed by 2\n\n\\[\nU_{n2t}= \\beta_{mf}Med.Farms_{n2t}+\\beta_{sf}SmallFarms_{n2t} \\\\\n+\\beta_{mh}Med.Height_{n2t}+\\beta_{lh}LowHeight_{n2t} \\\\\n+\\beta_{rk}redKite_{n2t}+\\beta_{md}MinDistance_{n2t} \\\\\n\\beta_{cost}Cost_{n2t}+ \\epsilon_{n2t}\n\\]\n\nProgram C will be alternative 3 and thus is indexed by 3\n\\[\nU_{n3t}= \\beta_{mf}Med.Farms_{n3t}+\\beta_{sf}SmallFarms_{n3t} \\\\\n+\\beta_{mh}Med.Height_{n3t}+\\beta_{lh}LowHeight_{n3t} \\\\\n+\\beta_{rk}redKite_{n3t}+\\beta_{md}MinDistance_{n3t} \\\\\n\\beta_{cost}Cost_{n3t}+ \\epsilon_{n3t}\n\\]"
  },
  {
    "objectID": "sp.html#experiential-design",
    "href": "sp.html#experiential-design",
    "title": "Stated Preference",
    "section": "Experiential Design",
    "text": "Experiential Design\nWe will now look at a full factorial design for the entire choice set and all the levels. Given the choices above the amount of possible combinations balloons to 5mil+ observations!\n\n# Create the full factorial using a named list of attributes and levels in the wide format\nfull_fact &lt;- full_factorial( list( alt1_sq = 1,\nalt1_farm = 0,\nalt1_height = 0,\nalt1_redkite = 0,\nalt1_distance = 0,\nalt1_cost = 0,\nalt2_sq = 0,\nalt2_farm = c(1, 2, 3),\nalt2_height = c(1, 2, 3),\nalt2_redkite = c(-5, -2.5, 0, 2.5, 5), alt2_distance = c(0, 0.25, 0.5, 0.75, 1), alt2_cost = 1:10,\nalt3_sq = 0,\nalt3_farm = c(1, 2, 3),\nalt3_height = c(1, 2, 3),\nalt3_redkite = c(-5, -2.5, 0, 2.5, 5), alt3_distance = c(0, 0.25, 0.5, 0.75, 1), alt3_cost = 1:10\n) )\n\n# Show the first six rows and 8th to 12th columns of the design matrix\nfull_fact[1:6, c(1, 8:12)]\n\n  alt1_sq alt2_farm alt2_height alt2_redkite alt2_distance alt2_cost\n1       1         1           1           -5             0         1\n2       1         2           1           -5             0         1\n3       1         3           1           -5             0         1\n4       1         1           2           -5             0         1\n5       1         2           2           -5             0         1\n6       1         3           2           -5             0         1\n\n\nAs the number of attributes, levels, and alternatives increases, full factorial designs become less practical for several reasons:\n\nDuplicate alternatives: Some choice tasks may repeat the same alternative, which doesn’t help us learn anything new about preferences.\nDominated alternatives: Some options in a choice task might be clearly worse (or better) than others in every way. These don’t help reveal trade-offs because people will always pick the best one, making the data less useful.\nLack of control: The full factorial includes all possible combinations, even unrealistic ones. For example, we might want to prevent small wind farms from showing up with the highest red kite impact.\n\n\nLogical Operators\nLets say we want to put restriction by putting logical restrictions. For example, the tall windmills cannot be placed too close to residential areas (this could already be a law and thus is a more accurate reflection of reality).\n\ncandidate_set &lt;- full_fact[!((full_fact$alt2_height == 1 & full_fact$alt2_distance &lt; 0.75) | (full_fact$alt3_height == 1 & full_fact$alt3_distance &lt; 0.75)), ]\n\ncandidate_set[1:6, c(1, 8:12)]\n\n     alt1_sq alt2_farm alt2_height alt2_redkite alt2_distance alt2_cost\n6754       1         1           2           -5             0         1\n6755       1         2           2           -5             0         1\n6756       1         3           2           -5             0         1\n6757       1         1           3           -5             0         1\n6758       1         2           3           -5             0         1\n6759       1         3           3           -5             0         1\n\n\nThis reduces the number of observations to 3.2million+ but does not make our choice set reasonable for population sampling. There for we move onto the next approach D-efficient\n\n\nD-efficient Design\nIn statistics, we often try to reduce standard errors to improve the precision of our estimates. The same idea applies in Discrete Choice Experiments (DCEs). We want to design choice tasks that give us the most precise information.\nThink of it this way:\n\nWhen fitting a model, we already have the data and estimate the parameters that best explain it.\nWhen designing a DCE, we do the reverse: we assume values for the parameters (called priors) and then search for the combination of attributes and levels that will give us the most information — that is, the lowest standard errors or lowest D-error.\n\n\n\nUtility Function\nFor our example we need to design a utility function to estimate the best set of potential choice cards. The utility function was written out within choice set section. So here we are going to use the library spdesign to write out each alternative.\n\nutility &lt;- list(\nalt1 = \"b_sq[0] * sq[1]\",\nalt2 = \"b_farm_dummy[c(0.25, 0.5)] * farm[c(1, 2, 3)] +\nb_height_dummy[c(0.25, 0.5)] * height[c(1, 2, 3)] + b_redkite[-0.05] * redkite[c(-5, -2.5, 0, 2.5, 5)] + b_distance[0.5] * distance[c(0, 0.25, 0.5, 0.75, 1)] + b_cost[-0.05] * cost[seq(1, 10)]\",\nalt3 = \"b_farm_dummy * farm + b_height_dummy * height +\nb_redkite * redkite + b_distance * distance + b_cost * cost\"\n)\n\n\n\nGenerating Design\nIn library spdesign generate_designis a function that generates efficient experimental designs. The function takes a set of indirect utility functions and generates efficient experimental designs assuming that people are maximizing utility.\nHere are the arguments needed for our example:\n\n\n\n\n\n\n\nutility\nA named list of utility functions. See the examples and the vignette for examples of how to define these correctly for different types of experimental designs.\n\n\nrows\nAn integer giving the number of rows in the final design\n\n\nmodel\nA character string indicating the model to optimize the design for. Currently the only model programmed is the ‘mnl’ model and this is also set as the default.\n\n\nefficiency_criteria\nA character string giving the efficiency criteria to optimize for. One of ‘a-error’, ‘c-error’, ‘d-error’ or ‘s-error’. No default is set and argument must be specified. Optimizing for multiple criteria is not yet implemented and will result in an error.\n\n\nalgorithm\nA character string giving the optimization algorithm to use. No default is set and the argument must be specified to be one of ‘rsc’, ‘federov’ or ‘random’.\n\n\n\n\n# Generate design ----\ndesign &lt;- generate_design(utility, rows = 100,\nmodel = \"mnl\", efficiency_criteria = \"d-error\", algorithm = \"rsc\")\n\n\n\n\n── Checking function arguments ──\n\n\n\n\n\nℹ The cycling part of the algorithm is not used. It only applies to a\nsmall subset of designs. The algorithm swithes between relabeling of\nattribute levels and swapping of attributes.\n\n\n\n\n\n── Preparing the list of priors ──\n\n\n\n\n\n✔ Priors prepared successfully\n\n\n\n\n\n── Evaluating designs ──────────────────────────────────────────────────────────\n\n\n\n──────────────────────────────────────────────────────────────────────────────── \n Iteration   A-error   C-error   D-error   S-error               Time stamp\n──────────────────────────────────────────────────────────────────────────────── \n         1    0.1412       N/A    0.0429       Inf2025-10-05 20:48:19.014901\n──────────────────────────────────────────────────────────────────────────── \n\n\nℹ Efficiency criteria is less than threshhold.\n\n\n\n\n\n── Cleaning up design environment ──────────────────────────────────────────────\n\n\nTime spent searching for designs:  0.04386878 \n\n\n\nsummary(design)\n\n---------------------------------------------------------------------\nAn 'spdesign' object\n\nUtility functions:\nalt1 : b_sq * alt1_sq \nalt2 : b_farm_dummy * alt2_farm + b_height_dummy * alt2_height + b_redkite * alt2_redkite + b_distance * alt2_distance + b_cost * alt2_cost \nalt3 : b_farm_dummy * alt3_farm + b_height_dummy * alt3_height + b_redkite * alt3_redkite + b_distance * alt3_distance + b_cost * alt3_cost \n\n\n   a-error    c-error    d-error    s-error \n0.14121250        NaN 0.04290615        Inf \n\n---------------------------------------------------------------------\n\nPrinting the first few rows of the design \n# A tibble: 6 × 15\n  alt1_sq alt2_farm2 alt2_farm3 alt2_height2 alt2_height3 alt2_redkite\n    &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1       1          0          0            0            0          0  \n2       1          1          0            0            0          2.5\n3       1          0          1            0            0         -5  \n4       1          0          0            0            1          2.5\n5       1          1          0            1            0         -5  \n6       1          0          1            1            0         -2.5\n# ℹ 9 more variables: alt2_distance &lt;dbl&gt;, alt2_cost &lt;dbl&gt;, alt3_farm2 &lt;dbl&gt;,\n#   alt3_farm3 &lt;dbl&gt;, alt3_height2 &lt;dbl&gt;, alt3_height3 &lt;dbl&gt;,\n#   alt3_redkite &lt;dbl&gt;, alt3_distance &lt;dbl&gt;, alt3_cost &lt;dbl&gt;\n\n---------------------------------------------------------------------\n\n\n\n\nCorrelation\nNext step check correlation\n\n# Correlation matrix\ncor(design)\n\nWarning in stats::cor(x[[\"design\"]], y = NULL, use = \"everything\", method =\nc(\"pearson\", : the standard deviation is zero\n\n\n              alt1_sq  alt2_farm2   alt2_farm3 alt2_height2  alt2_height3\nalt1_sq             1          NA           NA           NA            NA\nalt2_farm2         NA  1.00000000 -0.503717523 -0.054771424  3.501780e-02\nalt2_farm3         NA -0.50371752  1.000000000  0.140660335 -8.548168e-02\nalt2_height2       NA -0.05477142  0.140660335  1.000000000 -4.925373e-01\nalt2_height3       NA  0.03501780 -0.085481682 -0.492537313  1.000000e+00\nalt2_redkite       NA -0.08956222  0.090228114 -0.105266133 -5.342578e-18\nalt2_distance      NA -0.11941629 -0.030076038 -0.075190095 -3.007604e-02\nalt2_cost          NA  0.07349564  0.033318939 -0.136977862  2.591473e-02\nalt3_farm2         NA  0.03501780  0.004975124 -0.266395296  2.311171e-01\nalt3_farm3         NA -0.09966603 -0.085481682  0.140660335 -1.307101e-01\nalt3_height2       NA -0.02495544 -0.099666034  0.124807016 -1.445606e-01\nalt3_height3       NA  0.16970163 -0.085481682  0.004975124  9.543193e-02\nalt3_redkite       NA -0.02985407 -0.135342172 -0.030076038 -7.519010e-02\nalt3_distance      NA -0.16419739  0.135342172 -0.060152076  6.015208e-02\nalt3_cost          NA -0.01469913  0.173998906 -0.099956818  8.514840e-02\n               alt2_redkite alt2_distance   alt2_cost   alt3_farm2   alt3_farm3\nalt1_sq                  NA            NA          NA           NA           NA\nalt2_farm2    -8.956222e-02   -0.11941629  0.07349564  0.035017796 -0.099666034\nalt2_farm3     9.022811e-02   -0.03007604  0.03331894  0.004975124 -0.085481682\nalt2_height2  -1.052661e-01   -0.07519010 -0.13697786 -0.266395296  0.140660335\nalt2_height3  -5.342578e-18   -0.03007604  0.02591473  0.231117142 -0.130710086\nalt2_redkite   1.000000e+00    0.08000000  0.08124038 -0.075190095  0.060152076\nalt2_distance  8.000000e-02    1.00000000  0.07631672  0.060152076  0.120304152\nalt2_cost      8.124038e-02    0.07631672  1.00000000  0.144382071 -0.033318939\nalt3_farm2    -7.519010e-02    0.06015208  0.14438207  1.000000000 -0.492537313\nalt3_farm3     6.015208e-02    0.12030415 -0.03331894 -0.492537313  1.000000000\nalt3_height2  -4.478111e-02    0.01492704 -0.18373911 -0.054771424  0.079912406\nalt3_height3  -9.022811e-02   -0.12030415 -0.03331894 -0.130710086  0.004975124\nalt3_redkite  -1.600000e-01   -0.14000000 -0.08370221  0.150380191 -0.075190095\nalt3_distance -4.000000e-02    0.12000000 -0.02708013  0.240608305 -0.135342172\nalt3_cost      2.412593e-01    0.12801515  0.17696970 -0.062935774  0.003702104\n              alt3_height2 alt3_height3 alt3_redkite alt3_distance    alt3_cost\nalt1_sq                 NA           NA           NA            NA           NA\nalt2_farm2     -0.02495544  0.169701625  -0.02985407   -0.16419739 -0.014699129\nalt2_farm3     -0.09966603 -0.085481682  -0.13534217    0.13534217  0.173998906\nalt2_height2    0.12480702  0.004975124  -0.03007604   -0.06015208 -0.099956818\nalt2_height3   -0.14456064  0.095431931  -0.07519010    0.06015208  0.085148401\nalt2_redkite   -0.04478111 -0.090228114  -0.16000000   -0.04000000  0.241259322\nalt2_distance   0.01492704 -0.120304152  -0.14000000    0.12000000  0.128015151\nalt2_cost      -0.18373911 -0.033318939  -0.08370221   -0.02708013  0.176969697\nalt3_farm2     -0.05477142 -0.130710086   0.15038019    0.24060830 -0.062935774\nalt3_farm3      0.07991241  0.004975124  -0.07519010   -0.13534217  0.003702104\nalt3_height2    1.00000000 -0.503717523   0.16419739   -0.11941629 -0.110243466\nalt3_height3   -0.50371752  1.000000000   0.03007604   -0.09022811 -0.062935774\nalt3_redkite    0.16419739  0.030076038   1.00000000    0.16000000 -0.150171619\nalt3_distance  -0.11941629 -0.090228114   0.16000000    1.00000000 -0.046774767\nalt3_cost      -0.11024347 -0.062935774  -0.15017162   -0.04677477  1.000000000\n\n\n\n\nAttribute Balance\n\n# Print only the first three list elements\nlevel_balance(design)[1:3]\n\n$alt1_sq\n\n  1 \n100 \n\n$alt2_farm2\n\n 0  1 \n66 34 \n\n$alt2_farm3\n\n 0  1 \n67 33 \n\n\nFirst, we can see that the constant for the status quo alternative appears in all 100 rows of the design. Next, the medium and small wind farm sizes each occur 33 times, meaning the large size appears 34 times. This suggests the design is nearly balanced across attribute levels.\n\n\nDominated Strategy Check\nDominant or dominated alternatives should be avoided because they don’t provide useful information about trade-offs and can bias your results.\nTo check for this, we can look at the choice probabilities for each alternative. If one option has a probability close to 1, it likely dominates the others. If it’s close to 0, it’s probably dominated.\nThe spdesign package includes a probabilities() function that calculates these values based on your design and priors. It shows the probability of choosing each alternative in every choice task. Each row of the output adds up to 1.\n\n# Check the utility balance by inspecting the probabilities. We use head() to avoid printing all 100 rows in the book.\nprobabilities(design) |&gt;\nhead()\n\n          alt1      alt2      alt3\n[1,] 0.3104271 0.2808861 0.4086868\n[2,] 0.2898820 0.3816386 0.3284794\n[3,] 0.2702708 0.4456013 0.2841279\n[4,] 0.2282448 0.3004912 0.4712640\n[5,] 0.2416247 0.4402689 0.3181063\n[6,] 0.1918529 0.4838277 0.3243194"
  },
  {
    "objectID": "dce.html",
    "href": "dce.html",
    "title": "Discrete Choice Experiment",
    "section": "",
    "text": "Discrete Choice Experiments (DCEs) present respondents with several choice scenarios, each containing multiple alternatives described by various attributes and their levels. Respondents choose their preferred alternative in each scenario. R packages facilitate the design of DCEs (e.g., using orthogonal main-effect designs) and the analysis of choice data using models like conditional and binary logit.\nFor our example we will be using this book\nEnvironmental Valuation with Discrete Choice Experiments in R (Mariel et al. 2025)\n\nThese are the libraries you need to run the code below:\n\n\nShow code\n# Note if you don't have packages install.packages(\"put library name in here\")\n\n#install.packages(\"Rfast\")\n#install.packages(\"spdesign\")\n#install.packages(\"tidyr\")\n#install.packages(\"tibble\")\n\n\nlibrary(Rfast)\nlibrary(spdesign)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(tibble)\n\n\nWe will use the example the above book uses through out the chapters.\n\n\n\n\n\n\n\n\n\n\nAttributes\nLabels\nLevels\n\n\n\n\nSize of Wind Farm (discrete)\nSmall Farms\n0\n\n\nNote reference is LargeFarm\n\n1\n\n\n\nMediumFarms\n0\n\n\n\n\n1\n\n\nMax. Height Turbine (discrete)\nLow Height\n0\n\n\nNote reference is HighHeight\n\n1\n\n\n\nMedium Height\n0\n\n\n\n\n1\n\n\nReduction in Red Kite (continous)\nRed Kite\n5\n\n\n\n\n7.5\n\n\n\n\n10\n\n\n\n\n12.5\n\n\n\n\n15\n\n\nDistance to residents (continous)\nMinDistance\n750\n\n\n\n\n1000\n\n\n\n\n1250\n\n\n\n\n1500\n\n\n\n\n1750\n\n\nMonthlyCost\n(Continous)\nCost\n0\n\n\n\n\n1\n\n\n\n\n2\n\n\n\n\n3\n\n\n\n\n4\n\n\n\n\n…..\n\n\n\n\n….\n\n\n\n\n10\n\n\n\n\n\n\nLets first consider this example\n\nThis choice card has 3 alternatives and thus 3 different utility functions you are estimating:\n\nThe status quo or the opting out and keeping things the way they are. The utility function would look something like this:\n\n\\[\n\\begin{aligned}\nU_{n1t} =\\; & \\beta_{mf} \\, Med.Farms_{n1t} + \\beta_{sf} \\, SmallFarms_{n1t} \\\\& + \\beta_{mh} \\, Med.Height_{n1t} + \\beta_{lh}\n\\, LowHeight_{n1t} \\\\& + \\beta_{rk} \\, redKite_{n|t} + \\beta_{md} \\, MinDistance_{n1t} \\\\& + \\beta_{cost} \\, Cost_{n1t} + \\epsilon_{n1t}\n\\end{aligned}\n\\]\n\nProgram B will be alternative 2 and thus is indexed by 2\n\n\\[\n\\begin{aligned}\nU_{n2t}= \\beta_{mf}Med.Farms_{n2t}+\\beta_{sf}SmallFarms_{n2t} \\\\\n+\\beta_{mh}Med.Height_{n2t}+\\beta_{lh}LowHeight_{n2t} \\\\\n+\\beta_{rk}redKite_{n2t}+\\beta_{md}MinDistance_{n2t} \\\\\n\\beta_{cost}Cost_{n2t}+ \\epsilon_{n2t} \\\\\n\\end{aligned}\n\\]\n\nProgram C will be alternative 3 and thus is indexed by 3\n\\[\n\\begin{aligned}\nU_{n3t}= \\beta_{mf}Med.Farms_{n3t}+\\beta_{sf}SmallFarms_{n3t} \\\\\n+\\beta_{mh}Med.Height_{n3t}+\\beta_{lh}LowHeight_{n3t} \\\\\n+\\beta_{rk}redKite_{n3t}+\\beta_{md}MinDistance_{n3t} \\\\\n\\beta_{cost}Cost_{n3t}+ \\epsilon_{n3t} \\\\\n\\end{aligned}\n\\]\n\n\n\n\nWe will now look at a full factorial design for the entire choice set and all the levels. Given the choices above the amount of possible combinations balloons to 5mil+ observations!\n\n\nShow code\n# Create the full factorial using a named list of attributes and levels in the wide format\nfull_fact &lt;- full_factorial( list( alt1_sq = 1,\nalt1_farm = 0,\nalt1_height = 0,\nalt1_redkite = 0,\nalt1_distance = 0,\nalt1_cost = 0,\nalt2_sq = 0,\nalt2_farm = c(1, 2, 3),\nalt2_height = c(1, 2, 3),\nalt2_redkite = c(-5, -2.5, 0, 2.5, 5), alt2_distance = c(0, 0.25, 0.5, 0.75, 1), alt2_cost = 1:10,\nalt3_sq = 0,\nalt3_farm = c(1, 2, 3),\nalt3_height = c(1, 2, 3),\nalt3_redkite = c(-5, -2.5, 0, 2.5, 5), alt3_distance = c(0, 0.25, 0.5, 0.75, 1), alt3_cost = 1:10\n) )\n\n# Show the first six rows and 8th to 12th columns of the design matrix\nfull_fact[1:6, c(1, 8:12)]\n\n\n  alt1_sq alt2_farm alt2_height alt2_redkite alt2_distance alt2_cost\n1       1         1           1           -5             0         1\n2       1         2           1           -5             0         1\n3       1         3           1           -5             0         1\n4       1         1           2           -5             0         1\n5       1         2           2           -5             0         1\n6       1         3           2           -5             0         1\n\n\nAs the number of attributes, levels, and alternatives increases, full factorial designs become less practical for several reasons:\n\nDuplicate alternatives: Some choice tasks may repeat the same alternative, which doesn’t help us learn anything new about preferences.\nDominated alternatives: Some options in a choice task might be clearly worse (or better) than others in every way. These don’t help reveal trade-offs because people will always pick the best one, making the data less useful.\nLack of control: The full factorial includes all possible combinations, even unrealistic ones. For example, we might want to prevent small wind farms from showing up with the highest red kite impact.\n\n\n\nLets say we want to put restriction by putting logical restrictions. For example, the tall windmills cannot be placed too close to residential areas (this could already be a law and thus is a more accurate reflection of reality).\n\n\nShow code\ncandidate_set &lt;- full_fact[!((full_fact$alt2_height == 1 & full_fact$alt2_distance &lt; 0.75) | (full_fact$alt3_height == 1 & full_fact$alt3_distance &lt; 0.75)), ]\n\ncandidate_set[1:6, c(1, 8:12)]\n\n\n     alt1_sq alt2_farm alt2_height alt2_redkite alt2_distance alt2_cost\n6754       1         1           2           -5             0         1\n6755       1         2           2           -5             0         1\n6756       1         3           2           -5             0         1\n6757       1         1           3           -5             0         1\n6758       1         2           3           -5             0         1\n6759       1         3           3           -5             0         1\n\n\nThis reduces the number of observations to 3.2million+ but does not make our choice set reasonable for population sampling. There for we move onto the next approach D-efficient\n\n\n\nIn statistics, we often try to reduce standard errors to improve the precision of our estimates. The same idea applies in Discrete Choice Experiments (DCEs). We want to design choice tasks that give us the most precise information.\nThink of it this way:\n\nWhen fitting a model, we already have the data and estimate the parameters that best explain it.\nWhen designing a DCE, we do the reverse: we assume values for the parameters (called priors) and then search for the combination of attributes and levels that will give us the most information — that is, the lowest standard errors or lowest D-error.\n\n\n\n\nFor our example we need to design a utility function to estimate the best set of potential choice cards. The utility function was written out within choice set section. So here we are going to use the library spdesign to write out each alternative.\n\n\nShow code\nutility &lt;- list(\nalt1 = \"b_sq[0] * sq[1]\",\nalt2 = \"b_farm_dummy[c(0.25, 0.5)] * farm[c(1, 2, 3)] +\nb_height_dummy[c(0.25, 0.5)] * height[c(1, 2, 3)] + b_redkite[-0.05] * redkite[c(-5, -2.5, 0, 2.5, 5)] + b_distance[0.5] * distance[c(0, 0.25, 0.5, 0.75, 1)] + b_cost[-0.05] * cost[seq(1, 10)]\",\nalt3 = \"b_farm_dummy * farm + b_height_dummy * height +\nb_redkite * redkite + b_distance * distance + b_cost * cost\"\n)\n\n\n\n\n\nIn library spdesign generate_designis a function that generates efficient experimental designs. The function takes a set of indirect utility functions and generates efficient experimental designs assuming that people are maximizing utility.\nHere are the arguments needed for our example:\n\n\n\n\n\n\n\nutility\nA named list of utility functions. See the examples and the vignette for examples of how to define these correctly for different types of experimental designs.\n\n\nrows\nAn integer giving the number of rows in the final design\n\n\nmodel\nA character string indicating the model to optimize the design for. Currently the only model programmed is the ‘mnl’ model and this is also set as the default.\n\n\nefficiency_criteria\nA character string giving the efficiency criteria to optimize for. One of ‘a-error’, ‘c-error’, ‘d-error’ or ‘s-error’. No default is set and argument must be specified. Optimizing for multiple criteria is not yet implemented and will result in an error.\n\n\nalgorithm\nA character string giving the optimization algorithm to use. No default is set and the argument must be specified to be one of ‘rsc’, ‘federov’ or ‘random’.\n\n\n\n\n\nShow code\n# Generate design ----\ndesign &lt;- generate_design(utility, rows = 100,\nmodel = \"mnl\", efficiency_criteria = \"d-error\", algorithm = \"rsc\")\n\n\n\n\n\n── Checking function arguments ──\n\n\n\n\n\nℹ The cycling part of the algorithm is not used. It only applies to a\nsmall subset of designs. The algorithm swithes between relabeling of\nattribute levels and swapping of attributes.\n\n\n\n\n\n── Preparing the list of priors ──\n\n\n\n\n\n✔ Priors prepared successfully\n\n\n\n\n\n── Evaluating designs ──────────────────────────────────────────────────────────\n\n\n\n──────────────────────────────────────────────────────────────────────────────── \n Iteration   A-error   C-error   D-error   S-error               Time stamp\n──────────────────────────────────────────────────────────────────────────────── \n         1    0.1403       N/A    0.0436       Inf2025-11-14 15:27:09.459357\n──────────────────────────────────────────────────────────────────────────── \n\n\nℹ Efficiency criteria is less than threshhold.\n\n\n\n\n\n── Cleaning up design environment ──────────────────────────────────────────────\n\n\nTime spent searching for designs:  0.08929706 \n\n\n\n\nShow code\nsummary(design)\n\n\n---------------------------------------------------------------------\nAn 'spdesign' object\n\nUtility functions:\nalt1 : b_sq * alt1_sq \nalt2 : b_farm_dummy * alt2_farm + b_height_dummy * alt2_height + b_redkite * alt2_redkite + b_distance * alt2_distance + b_cost * alt2_cost \nalt3 : b_farm_dummy * alt3_farm + b_height_dummy * alt3_height + b_redkite * alt3_redkite + b_distance * alt3_distance + b_cost * alt3_cost \n\n\n   a-error    c-error    d-error    s-error \n0.14025940        NaN 0.04356665        Inf \n\n---------------------------------------------------------------------\n\nPrinting the first few rows of the design \n# A tibble: 6 × 15\n  alt1_sq alt2_farm2 alt2_farm3 alt2_height2 alt2_height3 alt2_redkite\n    &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1       1          0          1            0            0         -5  \n2       1          0          1            1            0         -2.5\n3       1          1          0            0            0          0  \n4       1          0          1            0            1         -2.5\n5       1          0          1            0            0          2.5\n6       1          1          0            1            0          2.5\n# ℹ 9 more variables: alt2_distance &lt;dbl&gt;, alt2_cost &lt;dbl&gt;, alt3_farm2 &lt;dbl&gt;,\n#   alt3_farm3 &lt;dbl&gt;, alt3_height2 &lt;dbl&gt;, alt3_height3 &lt;dbl&gt;,\n#   alt3_redkite &lt;dbl&gt;, alt3_distance &lt;dbl&gt;, alt3_cost &lt;dbl&gt;\n\n---------------------------------------------------------------------\n\n\n\n\n\nNext step check correlation\n\n\nShow code\n# Correlation matrix\ncor(design)\n\n\nWarning in stats::cor(x[[\"design\"]], y = NULL, use = \"everything\", method =\nc(\"pearson\", : the standard deviation is zero\n\n\n              alt1_sq  alt2_farm2    alt2_farm3 alt2_height2  alt2_height3\nalt1_sq             1          NA            NA           NA            NA\nalt2_farm2         NA  1.00000000 -5.037175e-01  -0.17593849  1.248070e-01\nalt2_farm3         NA -0.50371752  1.000000e+00   0.07991241  1.960784e-02\nalt2_height2       NA -0.17593849  7.991241e-02   1.00000000 -5.037175e-01\nalt2_height3       NA  0.12480702  1.960784e-02  -0.50371752  1.000000e+00\nalt2_redkite       NA  0.07519010 -1.325787e-18   0.04511406  1.343433e-01\nalt2_distance      NA -0.04511406 -1.492704e-02   0.25564632 -1.343433e-01\nalt2_cost          NA -0.01851052  1.837391e-01  -0.19621153 -2.447897e-18\nalt3_farm2         NA  0.12480702 -6.951872e-02  -0.14456064  1.532977e-01\nalt3_farm3         NA -0.08548168  1.697016e-01   0.05020353 -5.477142e-02\nalt3_height2       NA  0.23111714 -1.894553e-01  -0.08548168  3.501780e-02\nalt3_height3       NA -0.08548168  7.991241e-02   0.23111714 -9.966603e-02\nalt3_redkite       NA -0.03007604  5.970814e-02   0.19549425 -1.044893e-01\nalt3_distance      NA  0.15038019 -8.956222e-02   0.13534217  1.492704e-02\nalt3_cost          NA  0.14438207 -7.349564e-02   0.02591473  5.144695e-02\n               alt2_redkite alt2_distance     alt2_cost  alt3_farm2  alt3_farm3\nalt1_sq                  NA            NA            NA          NA          NA\nalt2_farm2     7.519010e-02   -0.04511406 -1.851052e-02  0.12480702 -0.08548168\nalt2_farm3    -1.325787e-18   -0.01492704  1.837391e-01 -0.06951872  0.16970163\nalt2_height2   4.511406e-02    0.25564632 -1.962115e-01 -0.14456064  0.05020353\nalt2_height3   1.343433e-01   -0.13434332 -2.447897e-18  0.15329768 -0.05477142\nalt2_redkite   1.000000e+00    0.02500000  2.708013e-02 -0.20897850  0.03007604\nalt2_distance  2.500000e-02    1.00000000 -5.169843e-02 -0.14927036  0.16541821\nalt2_cost      2.708013e-02   -0.05169843  1.000000e+00 -0.01469913 -0.02591473\nalt3_farm2    -2.089785e-01   -0.14927036 -1.469913e-02  1.00000000 -0.50371752\nalt3_farm3     3.007604e-02    0.16541821 -2.591473e-02 -0.50371752  1.00000000\nalt3_height2  -1.503802e-02   -0.07519010 -1.851052e-02  0.25949085 -0.13071009\nalt3_height3   1.503802e-01    0.01503802 -1.517863e-01 -0.14456064 -0.08548168\nalt3_redkite  -2.000000e-02    0.00500000  9.847319e-03 -0.14927036  0.12030415\nalt3_distance  3.350000e-01    0.04000000 -2.387975e-01 -0.13434332  0.01503802\nalt3_cost     -1.797136e-01    0.02461830  1.878788e-01  0.17638955 -0.13697786\n              alt3_height2 alt3_height3 alt3_redkite alt3_distance   alt3_cost\nalt1_sq                 NA           NA           NA            NA          NA\nalt2_farm2      0.23111714  -0.08548168 -0.030076038    0.15038019  0.14438207\nalt2_farm3     -0.18945525   0.07991241  0.059708143   -0.08956222 -0.07349564\nalt2_height2   -0.08548168   0.23111714  0.195494248    0.13534217  0.02591473\nalt2_height3    0.03501780  -0.09966603 -0.104489251    0.01492704  0.05144695\nalt2_redkite   -0.01503802   0.15038019 -0.020000000    0.33500000 -0.17971358\nalt2_distance  -0.07519010   0.01503802  0.005000000    0.04000000  0.02461830\nalt2_cost      -0.01851052  -0.15178628  0.009847319   -0.23879749  0.18787879\nalt3_farm2      0.25949085  -0.14456064 -0.149270359   -0.13434332  0.17638955\nalt3_farm3     -0.13071009  -0.08548168  0.120304152    0.01503802 -0.13697786\nalt3_height2    1.00000000  -0.49253731  0.090228114    0.03007604 -0.09255261\nalt3_height3   -0.49253731   1.00000000 -0.030076038    0.04511406 -0.04812736\nalt3_redkite    0.09022811  -0.03007604  1.000000000    0.21500000 -0.13293881\nalt3_distance   0.03007604   0.04511406  0.215000000    1.00000000 -0.35696532\nalt3_cost      -0.09255261  -0.04812736 -0.132938810   -0.35696532  1.00000000\n\n\n\n\n\n\n\nShow code\n# Print only the first three list elements\nlevel_balance(design)[1:3]\n\n\n$alt1_sq\n\n  1 \n100 \n\n$alt2_farm2\n\n 0  1 \n67 33 \n\n$alt2_farm3\n\n 0  1 \n66 34 \n\n\nFirst, we can see that the constant for the status quo alternative appears in all 100 rows of the design. Next, the medium and small wind farm sizes each occur 33 times, meaning the large size appears 34 times. This suggests the design is nearly balanced across attribute levels.\n\n\n\nDominant or dominated alternatives should be avoided because they don’t provide useful information about trade-offs and can bias your results.\nTo check for this, we can look at the choice probabilities for each alternative. If one option has a probability close to 1, it likely dominates the others. If it’s close to 0, it’s probably dominated.\nThe spdesign package includes a probabilities() function that calculates these values based on your design and priors. It shows the probability of choosing each alternative in every choice task. Each row of the output adds up to 1.\n\n\nShow code\n# Check the utility balance by inspecting the probabilities. We use head() to avoid printing all 100 rows in the book.\nprobabilities(design) |&gt;\nhead()\n\n\n          alt1      alt2      alt3\n[1,] 0.2304335 0.5391330 0.2304335\n[2,] 0.1731569 0.5749012 0.2519418\n[3,] 0.2715939 0.2457483 0.4826578\n[4,] 0.2052660 0.5176538 0.2770801\n[5,] 0.2601757 0.2537520 0.4860723\n[6,] 0.2249652 0.3441047 0.4309301\n\n\nTo help spot any problematic choice tasks, we can create a simple plot. In this case, the plot shows no signs of dominating or dominated alternatives which would appear as very large or very small segments of a single color.\nThe status quo option (shown in red) has a low probability of being chosen, but it’s not too low to be a problem. What’s considered “too low” depends on the context. For example, in labelled experiments, some options are naturally chosen less often, especially if they represent less common situations.\nIf the status quo is chosen too often or too rarely compared to your expectations, you should adjust its prior value:\n\nIncrease the prior if you expect more people to choose the status quo.\nDecrease it if you expect fewer to choose it.\n\nThis step highlights why it’s important to check your design and make sure the priors match what you expect from real-world behavior.\n\n\nShow code\n# Create a plot to show the choice shares across the design\nprobabilities(design) |&gt;\nas_tibble() |&gt;\nrowid_to_column() |&gt;\npivot_longer(-rowid, names_to = \"alt\", values_to = \"prob\") |&gt; ggplot(aes(x = rowid, y = prob, fill = alt)) + geom_bar(position = \"fill\", stat = \"identity\") +\nlabs(x = \"Choice task\", y = \"Choice probability\", fill = \"Alternative\") + scale_x_continuous(breaks = seq(1, 100, by = 2)) + scale_fill_discrete(label = c(\"SQ\", \"Alt 1\", \"Alt 2\")) +\ntheme_bw() +\ntheme(\nlegend.position = \"bottom\", axis.text.x = element_text(angle = 315)\n)\n\n\n\n\n\n\n\n\n\n\n\n\nSo next on the list would be check utility balance of each choice task in our design.\n\n\nShow code\nutility_balance &lt;- function(x) {\n#Ensure that it is a matrix (and not a data.frame()/tibble()) \nx &lt;- as.matrix(x)\n\n# Find number of non-zero alternatives where 0 or NA can be non-available\nn_alts &lt;- apply(x, 1, function(y) sum(y &gt; 0, na.rm = TRUE)) \n\n# Calculate for each alternative\nx &lt;- x / (1 / n_alts)\n\n#Replace all zeroes with 1 to enable taking the product\nindex_zero &lt;- x == 0 \nx[index_zero] = 1\n\n# Take the product. This line requires the Rfast package.\nx &lt;- Rfast::rowprods(x) \nreturn(x)\n}\n\n# Use the function for utility balance on the choice probabilities\nutility_balance(probabilities(design)) |&gt; \nhead()\n\n\n[1] 0.7729491 0.6771692 0.8697884 0.7949241 0.8664447 0.9006925\n\n\nThe function returns the utility balance for each choice task. The average utility balance across the design is 0.8478. For efficient designs, values typically fall between 70% and 90% indicates a good balance, not too equal (which gives little information) and not too skewed to have dominant alternatives.\n\n\n\nThe design created in this includes 100 choice tasks which is far too many for a single respondent to handle effectively. To address this, we present two common solutions, starting with the most widely used: blocking.\nBlocking involves dividing the full design into smaller subsets, or blocks, so that each respondent is only shown the tasks from one block. For example, if pre-testing shows that respondents can comfortably complete 10 tasks, then a 100-task design would be split into 10 blocks of 10 tasks each.\nEach choice task still needs to be answered by at least one respondent, so blocking increases the number of participants required. In this case, you’d need at least 10 respondents (one per block), instead of just one respondent completing all 100 tasks. In general, larger designs with blocking demand more respondents to ensure all tasks are adequately covered.\nWhen using a blocked design, each respondent is randomly assigned to a block, and the order of choice tasks within that block is also randomized. Be sure to record the specific choice tasks shown to each respondent so you can accurately reconstruct their responses later.\nThe blocking column in your design must be orthogonal, meaning it should not be correlated with the other attributes. The block() function from the spdesign package creates a blocking column that minimizes mean squared correlation. However, it does not preserve attribute level balance within each block.\nIf your overall design is balanced, blocking won’t change that. But keep in mind that in a blocked design, some respondents may never see certain attribute levels, which could affect how realistic the choice tasks feel. Also, depending on the complexity of your design, generating the blocking column may take some time.\n\n\nShow code\n# Add a blocking variable to the design with 10 blocks.\ndesign &lt;- block(design, 10)\n\n\nWarning in stats::cor(design, block): the standard deviation is zero\n\n\n\n\nShow code\ndesign$blocks_correlation\n\n\n# A tibble: 1 × 15\n  alt1_sq alt2_farm2 alt2_farm3 alt2_height2 alt2_height3 alt2_redkite\n    &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1      NA     0.0185   -0.00735      0.00370      -0.0367       0.0492\n# ℹ 9 more variables: alt2_distance &lt;dbl&gt;, alt2_cost &lt;dbl&gt;, alt3_farm2 &lt;dbl&gt;,\n#   alt3_farm3 &lt;dbl&gt;, alt3_height2 &lt;dbl&gt;, alt3_height3 &lt;dbl&gt;,\n#   alt3_redkite &lt;dbl&gt;, alt3_distance &lt;dbl&gt;, alt3_cost &lt;dbl&gt;\n\n\nHere, we see that the blocking column is practically uncorrelated with the rest of the design."
  },
  {
    "objectID": "dce.html#attribute-levels",
    "href": "dce.html#attribute-levels",
    "title": "Discrete Choice Experiment",
    "section": "",
    "text": "Attributes\nLabels\nLevels\n\n\n\n\nSize of Wind Farm (discrete)\nSmall Farms\n0\n\n\nNote reference is LargeFarm\n\n1\n\n\n\nMediumFarms\n0\n\n\n\n\n1\n\n\nMax. Height Turbine (discrete)\nLow Height\n0\n\n\nNote reference is HighHeight\n\n1\n\n\n\nMedium Height\n0\n\n\n\n\n1\n\n\nReduction in Red Kite (continous)\nRed Kite\n5\n\n\n\n\n7.5\n\n\n\n\n10\n\n\n\n\n12.5\n\n\n\n\n15\n\n\nDistance to residents (continous)\nMinDistance\n750\n\n\n\n\n1000\n\n\n\n\n1250\n\n\n\n\n1500\n\n\n\n\n1750\n\n\nMonthlyCost\n(Continous)\nCost\n0\n\n\n\n\n1\n\n\n\n\n2\n\n\n\n\n3\n\n\n\n\n4\n\n\n\n\n…..\n\n\n\n\n….\n\n\n\n\n10"
  },
  {
    "objectID": "dce.html#choice-set",
    "href": "dce.html#choice-set",
    "title": "Discrete Choice Experiment",
    "section": "",
    "text": "Lets first consider this example\n\nThis choice card has 3 alternatives and thus 3 different utility functions you are estimating:\n\nThe status quo or the opting out and keeping things the way they are. The utility function would look something like this:\n\n\\[\n\\begin{aligned}\nU_{n1t} =\\; & \\beta_{mf} \\, Med.Farms_{n1t} + \\beta_{sf} \\, SmallFarms_{n1t} \\\\& + \\beta_{mh} \\, Med.Height_{n1t} + \\beta_{lh}\n\\, LowHeight_{n1t} \\\\& + \\beta_{rk} \\, redKite_{n|t} + \\beta_{md} \\, MinDistance_{n1t} \\\\& + \\beta_{cost} \\, Cost_{n1t} + \\epsilon_{n1t}\n\\end{aligned}\n\\]\n\nProgram B will be alternative 2 and thus is indexed by 2\n\n\\[\n\\begin{aligned}\nU_{n2t}= \\beta_{mf}Med.Farms_{n2t}+\\beta_{sf}SmallFarms_{n2t} \\\\\n+\\beta_{mh}Med.Height_{n2t}+\\beta_{lh}LowHeight_{n2t} \\\\\n+\\beta_{rk}redKite_{n2t}+\\beta_{md}MinDistance_{n2t} \\\\\n\\beta_{cost}Cost_{n2t}+ \\epsilon_{n2t} \\\\\n\\end{aligned}\n\\]\n\nProgram C will be alternative 3 and thus is indexed by 3\n\\[\n\\begin{aligned}\nU_{n3t}= \\beta_{mf}Med.Farms_{n3t}+\\beta_{sf}SmallFarms_{n3t} \\\\\n+\\beta_{mh}Med.Height_{n3t}+\\beta_{lh}LowHeight_{n3t} \\\\\n+\\beta_{rk}redKite_{n3t}+\\beta_{md}MinDistance_{n3t} \\\\\n\\beta_{cost}Cost_{n3t}+ \\epsilon_{n3t} \\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "dce.html#experiential-design",
    "href": "dce.html#experiential-design",
    "title": "Discrete Choice Experiment",
    "section": "",
    "text": "We will now look at a full factorial design for the entire choice set and all the levels. Given the choices above the amount of possible combinations balloons to 5mil+ observations!\n\n\nShow code\n# Create the full factorial using a named list of attributes and levels in the wide format\nfull_fact &lt;- full_factorial( list( alt1_sq = 1,\nalt1_farm = 0,\nalt1_height = 0,\nalt1_redkite = 0,\nalt1_distance = 0,\nalt1_cost = 0,\nalt2_sq = 0,\nalt2_farm = c(1, 2, 3),\nalt2_height = c(1, 2, 3),\nalt2_redkite = c(-5, -2.5, 0, 2.5, 5), alt2_distance = c(0, 0.25, 0.5, 0.75, 1), alt2_cost = 1:10,\nalt3_sq = 0,\nalt3_farm = c(1, 2, 3),\nalt3_height = c(1, 2, 3),\nalt3_redkite = c(-5, -2.5, 0, 2.5, 5), alt3_distance = c(0, 0.25, 0.5, 0.75, 1), alt3_cost = 1:10\n) )\n\n# Show the first six rows and 8th to 12th columns of the design matrix\nfull_fact[1:6, c(1, 8:12)]\n\n\n  alt1_sq alt2_farm alt2_height alt2_redkite alt2_distance alt2_cost\n1       1         1           1           -5             0         1\n2       1         2           1           -5             0         1\n3       1         3           1           -5             0         1\n4       1         1           2           -5             0         1\n5       1         2           2           -5             0         1\n6       1         3           2           -5             0         1\n\n\nAs the number of attributes, levels, and alternatives increases, full factorial designs become less practical for several reasons:\n\nDuplicate alternatives: Some choice tasks may repeat the same alternative, which doesn’t help us learn anything new about preferences.\nDominated alternatives: Some options in a choice task might be clearly worse (or better) than others in every way. These don’t help reveal trade-offs because people will always pick the best one, making the data less useful.\nLack of control: The full factorial includes all possible combinations, even unrealistic ones. For example, we might want to prevent small wind farms from showing up with the highest red kite impact.\n\n\n\nLets say we want to put restriction by putting logical restrictions. For example, the tall windmills cannot be placed too close to residential areas (this could already be a law and thus is a more accurate reflection of reality).\n\n\nShow code\ncandidate_set &lt;- full_fact[!((full_fact$alt2_height == 1 & full_fact$alt2_distance &lt; 0.75) | (full_fact$alt3_height == 1 & full_fact$alt3_distance &lt; 0.75)), ]\n\ncandidate_set[1:6, c(1, 8:12)]\n\n\n     alt1_sq alt2_farm alt2_height alt2_redkite alt2_distance alt2_cost\n6754       1         1           2           -5             0         1\n6755       1         2           2           -5             0         1\n6756       1         3           2           -5             0         1\n6757       1         1           3           -5             0         1\n6758       1         2           3           -5             0         1\n6759       1         3           3           -5             0         1\n\n\nThis reduces the number of observations to 3.2million+ but does not make our choice set reasonable for population sampling. There for we move onto the next approach D-efficient\n\n\n\nIn statistics, we often try to reduce standard errors to improve the precision of our estimates. The same idea applies in Discrete Choice Experiments (DCEs). We want to design choice tasks that give us the most precise information.\nThink of it this way:\n\nWhen fitting a model, we already have the data and estimate the parameters that best explain it.\nWhen designing a DCE, we do the reverse: we assume values for the parameters (called priors) and then search for the combination of attributes and levels that will give us the most information — that is, the lowest standard errors or lowest D-error.\n\n\n\n\nFor our example we need to design a utility function to estimate the best set of potential choice cards. The utility function was written out within choice set section. So here we are going to use the library spdesign to write out each alternative.\n\n\nShow code\nutility &lt;- list(\nalt1 = \"b_sq[0] * sq[1]\",\nalt2 = \"b_farm_dummy[c(0.25, 0.5)] * farm[c(1, 2, 3)] +\nb_height_dummy[c(0.25, 0.5)] * height[c(1, 2, 3)] + b_redkite[-0.05] * redkite[c(-5, -2.5, 0, 2.5, 5)] + b_distance[0.5] * distance[c(0, 0.25, 0.5, 0.75, 1)] + b_cost[-0.05] * cost[seq(1, 10)]\",\nalt3 = \"b_farm_dummy * farm + b_height_dummy * height +\nb_redkite * redkite + b_distance * distance + b_cost * cost\"\n)\n\n\n\n\n\nIn library spdesign generate_designis a function that generates efficient experimental designs. The function takes a set of indirect utility functions and generates efficient experimental designs assuming that people are maximizing utility.\nHere are the arguments needed for our example:\n\n\n\n\n\n\n\nutility\nA named list of utility functions. See the examples and the vignette for examples of how to define these correctly for different types of experimental designs.\n\n\nrows\nAn integer giving the number of rows in the final design\n\n\nmodel\nA character string indicating the model to optimize the design for. Currently the only model programmed is the ‘mnl’ model and this is also set as the default.\n\n\nefficiency_criteria\nA character string giving the efficiency criteria to optimize for. One of ‘a-error’, ‘c-error’, ‘d-error’ or ‘s-error’. No default is set and argument must be specified. Optimizing for multiple criteria is not yet implemented and will result in an error.\n\n\nalgorithm\nA character string giving the optimization algorithm to use. No default is set and the argument must be specified to be one of ‘rsc’, ‘federov’ or ‘random’.\n\n\n\n\n\nShow code\n# Generate design ----\ndesign &lt;- generate_design(utility, rows = 100,\nmodel = \"mnl\", efficiency_criteria = \"d-error\", algorithm = \"rsc\")\n\n\n\n\n\n── Checking function arguments ──\n\n\n\n\n\nℹ The cycling part of the algorithm is not used. It only applies to a\nsmall subset of designs. The algorithm swithes between relabeling of\nattribute levels and swapping of attributes.\n\n\n\n\n\n── Preparing the list of priors ──\n\n\n\n\n\n✔ Priors prepared successfully\n\n\n\n\n\n── Evaluating designs ──────────────────────────────────────────────────────────\n\n\n\n──────────────────────────────────────────────────────────────────────────────── \n Iteration   A-error   C-error   D-error   S-error               Time stamp\n──────────────────────────────────────────────────────────────────────────────── \n         1    0.1403       N/A    0.0436       Inf2025-11-14 15:27:09.459357\n──────────────────────────────────────────────────────────────────────────── \n\n\nℹ Efficiency criteria is less than threshhold.\n\n\n\n\n\n── Cleaning up design environment ──────────────────────────────────────────────\n\n\nTime spent searching for designs:  0.08929706 \n\n\n\n\nShow code\nsummary(design)\n\n\n---------------------------------------------------------------------\nAn 'spdesign' object\n\nUtility functions:\nalt1 : b_sq * alt1_sq \nalt2 : b_farm_dummy * alt2_farm + b_height_dummy * alt2_height + b_redkite * alt2_redkite + b_distance * alt2_distance + b_cost * alt2_cost \nalt3 : b_farm_dummy * alt3_farm + b_height_dummy * alt3_height + b_redkite * alt3_redkite + b_distance * alt3_distance + b_cost * alt3_cost \n\n\n   a-error    c-error    d-error    s-error \n0.14025940        NaN 0.04356665        Inf \n\n---------------------------------------------------------------------\n\nPrinting the first few rows of the design \n# A tibble: 6 × 15\n  alt1_sq alt2_farm2 alt2_farm3 alt2_height2 alt2_height3 alt2_redkite\n    &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1       1          0          1            0            0         -5  \n2       1          0          1            1            0         -2.5\n3       1          1          0            0            0          0  \n4       1          0          1            0            1         -2.5\n5       1          0          1            0            0          2.5\n6       1          1          0            1            0          2.5\n# ℹ 9 more variables: alt2_distance &lt;dbl&gt;, alt2_cost &lt;dbl&gt;, alt3_farm2 &lt;dbl&gt;,\n#   alt3_farm3 &lt;dbl&gt;, alt3_height2 &lt;dbl&gt;, alt3_height3 &lt;dbl&gt;,\n#   alt3_redkite &lt;dbl&gt;, alt3_distance &lt;dbl&gt;, alt3_cost &lt;dbl&gt;\n\n---------------------------------------------------------------------\n\n\n\n\n\nNext step check correlation\n\n\nShow code\n# Correlation matrix\ncor(design)\n\n\nWarning in stats::cor(x[[\"design\"]], y = NULL, use = \"everything\", method =\nc(\"pearson\", : the standard deviation is zero\n\n\n              alt1_sq  alt2_farm2    alt2_farm3 alt2_height2  alt2_height3\nalt1_sq             1          NA            NA           NA            NA\nalt2_farm2         NA  1.00000000 -5.037175e-01  -0.17593849  1.248070e-01\nalt2_farm3         NA -0.50371752  1.000000e+00   0.07991241  1.960784e-02\nalt2_height2       NA -0.17593849  7.991241e-02   1.00000000 -5.037175e-01\nalt2_height3       NA  0.12480702  1.960784e-02  -0.50371752  1.000000e+00\nalt2_redkite       NA  0.07519010 -1.325787e-18   0.04511406  1.343433e-01\nalt2_distance      NA -0.04511406 -1.492704e-02   0.25564632 -1.343433e-01\nalt2_cost          NA -0.01851052  1.837391e-01  -0.19621153 -2.447897e-18\nalt3_farm2         NA  0.12480702 -6.951872e-02  -0.14456064  1.532977e-01\nalt3_farm3         NA -0.08548168  1.697016e-01   0.05020353 -5.477142e-02\nalt3_height2       NA  0.23111714 -1.894553e-01  -0.08548168  3.501780e-02\nalt3_height3       NA -0.08548168  7.991241e-02   0.23111714 -9.966603e-02\nalt3_redkite       NA -0.03007604  5.970814e-02   0.19549425 -1.044893e-01\nalt3_distance      NA  0.15038019 -8.956222e-02   0.13534217  1.492704e-02\nalt3_cost          NA  0.14438207 -7.349564e-02   0.02591473  5.144695e-02\n               alt2_redkite alt2_distance     alt2_cost  alt3_farm2  alt3_farm3\nalt1_sq                  NA            NA            NA          NA          NA\nalt2_farm2     7.519010e-02   -0.04511406 -1.851052e-02  0.12480702 -0.08548168\nalt2_farm3    -1.325787e-18   -0.01492704  1.837391e-01 -0.06951872  0.16970163\nalt2_height2   4.511406e-02    0.25564632 -1.962115e-01 -0.14456064  0.05020353\nalt2_height3   1.343433e-01   -0.13434332 -2.447897e-18  0.15329768 -0.05477142\nalt2_redkite   1.000000e+00    0.02500000  2.708013e-02 -0.20897850  0.03007604\nalt2_distance  2.500000e-02    1.00000000 -5.169843e-02 -0.14927036  0.16541821\nalt2_cost      2.708013e-02   -0.05169843  1.000000e+00 -0.01469913 -0.02591473\nalt3_farm2    -2.089785e-01   -0.14927036 -1.469913e-02  1.00000000 -0.50371752\nalt3_farm3     3.007604e-02    0.16541821 -2.591473e-02 -0.50371752  1.00000000\nalt3_height2  -1.503802e-02   -0.07519010 -1.851052e-02  0.25949085 -0.13071009\nalt3_height3   1.503802e-01    0.01503802 -1.517863e-01 -0.14456064 -0.08548168\nalt3_redkite  -2.000000e-02    0.00500000  9.847319e-03 -0.14927036  0.12030415\nalt3_distance  3.350000e-01    0.04000000 -2.387975e-01 -0.13434332  0.01503802\nalt3_cost     -1.797136e-01    0.02461830  1.878788e-01  0.17638955 -0.13697786\n              alt3_height2 alt3_height3 alt3_redkite alt3_distance   alt3_cost\nalt1_sq                 NA           NA           NA            NA          NA\nalt2_farm2      0.23111714  -0.08548168 -0.030076038    0.15038019  0.14438207\nalt2_farm3     -0.18945525   0.07991241  0.059708143   -0.08956222 -0.07349564\nalt2_height2   -0.08548168   0.23111714  0.195494248    0.13534217  0.02591473\nalt2_height3    0.03501780  -0.09966603 -0.104489251    0.01492704  0.05144695\nalt2_redkite   -0.01503802   0.15038019 -0.020000000    0.33500000 -0.17971358\nalt2_distance  -0.07519010   0.01503802  0.005000000    0.04000000  0.02461830\nalt2_cost      -0.01851052  -0.15178628  0.009847319   -0.23879749  0.18787879\nalt3_farm2      0.25949085  -0.14456064 -0.149270359   -0.13434332  0.17638955\nalt3_farm3     -0.13071009  -0.08548168  0.120304152    0.01503802 -0.13697786\nalt3_height2    1.00000000  -0.49253731  0.090228114    0.03007604 -0.09255261\nalt3_height3   -0.49253731   1.00000000 -0.030076038    0.04511406 -0.04812736\nalt3_redkite    0.09022811  -0.03007604  1.000000000    0.21500000 -0.13293881\nalt3_distance   0.03007604   0.04511406  0.215000000    1.00000000 -0.35696532\nalt3_cost      -0.09255261  -0.04812736 -0.132938810   -0.35696532  1.00000000\n\n\n\n\n\n\n\nShow code\n# Print only the first three list elements\nlevel_balance(design)[1:3]\n\n\n$alt1_sq\n\n  1 \n100 \n\n$alt2_farm2\n\n 0  1 \n67 33 \n\n$alt2_farm3\n\n 0  1 \n66 34 \n\n\nFirst, we can see that the constant for the status quo alternative appears in all 100 rows of the design. Next, the medium and small wind farm sizes each occur 33 times, meaning the large size appears 34 times. This suggests the design is nearly balanced across attribute levels.\n\n\n\nDominant or dominated alternatives should be avoided because they don’t provide useful information about trade-offs and can bias your results.\nTo check for this, we can look at the choice probabilities for each alternative. If one option has a probability close to 1, it likely dominates the others. If it’s close to 0, it’s probably dominated.\nThe spdesign package includes a probabilities() function that calculates these values based on your design and priors. It shows the probability of choosing each alternative in every choice task. Each row of the output adds up to 1.\n\n\nShow code\n# Check the utility balance by inspecting the probabilities. We use head() to avoid printing all 100 rows in the book.\nprobabilities(design) |&gt;\nhead()\n\n\n          alt1      alt2      alt3\n[1,] 0.2304335 0.5391330 0.2304335\n[2,] 0.1731569 0.5749012 0.2519418\n[3,] 0.2715939 0.2457483 0.4826578\n[4,] 0.2052660 0.5176538 0.2770801\n[5,] 0.2601757 0.2537520 0.4860723\n[6,] 0.2249652 0.3441047 0.4309301\n\n\nTo help spot any problematic choice tasks, we can create a simple plot. In this case, the plot shows no signs of dominating or dominated alternatives which would appear as very large or very small segments of a single color.\nThe status quo option (shown in red) has a low probability of being chosen, but it’s not too low to be a problem. What’s considered “too low” depends on the context. For example, in labelled experiments, some options are naturally chosen less often, especially if they represent less common situations.\nIf the status quo is chosen too often or too rarely compared to your expectations, you should adjust its prior value:\n\nIncrease the prior if you expect more people to choose the status quo.\nDecrease it if you expect fewer to choose it.\n\nThis step highlights why it’s important to check your design and make sure the priors match what you expect from real-world behavior.\n\n\nShow code\n# Create a plot to show the choice shares across the design\nprobabilities(design) |&gt;\nas_tibble() |&gt;\nrowid_to_column() |&gt;\npivot_longer(-rowid, names_to = \"alt\", values_to = \"prob\") |&gt; ggplot(aes(x = rowid, y = prob, fill = alt)) + geom_bar(position = \"fill\", stat = \"identity\") +\nlabs(x = \"Choice task\", y = \"Choice probability\", fill = \"Alternative\") + scale_x_continuous(breaks = seq(1, 100, by = 2)) + scale_fill_discrete(label = c(\"SQ\", \"Alt 1\", \"Alt 2\")) +\ntheme_bw() +\ntheme(\nlegend.position = \"bottom\", axis.text.x = element_text(angle = 315)\n)\n\n\n\n\n\n\n\n\n\n\n\n\nSo next on the list would be check utility balance of each choice task in our design.\n\n\nShow code\nutility_balance &lt;- function(x) {\n#Ensure that it is a matrix (and not a data.frame()/tibble()) \nx &lt;- as.matrix(x)\n\n# Find number of non-zero alternatives where 0 or NA can be non-available\nn_alts &lt;- apply(x, 1, function(y) sum(y &gt; 0, na.rm = TRUE)) \n\n# Calculate for each alternative\nx &lt;- x / (1 / n_alts)\n\n#Replace all zeroes with 1 to enable taking the product\nindex_zero &lt;- x == 0 \nx[index_zero] = 1\n\n# Take the product. This line requires the Rfast package.\nx &lt;- Rfast::rowprods(x) \nreturn(x)\n}\n\n# Use the function for utility balance on the choice probabilities\nutility_balance(probabilities(design)) |&gt; \nhead()\n\n\n[1] 0.7729491 0.6771692 0.8697884 0.7949241 0.8664447 0.9006925\n\n\nThe function returns the utility balance for each choice task. The average utility balance across the design is 0.8478. For efficient designs, values typically fall between 70% and 90% indicates a good balance, not too equal (which gives little information) and not too skewed to have dominant alternatives.\n\n\n\nThe design created in this includes 100 choice tasks which is far too many for a single respondent to handle effectively. To address this, we present two common solutions, starting with the most widely used: blocking.\nBlocking involves dividing the full design into smaller subsets, or blocks, so that each respondent is only shown the tasks from one block. For example, if pre-testing shows that respondents can comfortably complete 10 tasks, then a 100-task design would be split into 10 blocks of 10 tasks each.\nEach choice task still needs to be answered by at least one respondent, so blocking increases the number of participants required. In this case, you’d need at least 10 respondents (one per block), instead of just one respondent completing all 100 tasks. In general, larger designs with blocking demand more respondents to ensure all tasks are adequately covered.\nWhen using a blocked design, each respondent is randomly assigned to a block, and the order of choice tasks within that block is also randomized. Be sure to record the specific choice tasks shown to each respondent so you can accurately reconstruct their responses later.\nThe blocking column in your design must be orthogonal, meaning it should not be correlated with the other attributes. The block() function from the spdesign package creates a blocking column that minimizes mean squared correlation. However, it does not preserve attribute level balance within each block.\nIf your overall design is balanced, blocking won’t change that. But keep in mind that in a blocked design, some respondents may never see certain attribute levels, which could affect how realistic the choice tasks feel. Also, depending on the complexity of your design, generating the blocking column may take some time.\n\n\nShow code\n# Add a blocking variable to the design with 10 blocks.\ndesign &lt;- block(design, 10)\n\n\nWarning in stats::cor(design, block): the standard deviation is zero\n\n\n\n\nShow code\ndesign$blocks_correlation\n\n\n# A tibble: 1 × 15\n  alt1_sq alt2_farm2 alt2_farm3 alt2_height2 alt2_height3 alt2_redkite\n    &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1      NA     0.0185   -0.00735      0.00370      -0.0367       0.0492\n# ℹ 9 more variables: alt2_distance &lt;dbl&gt;, alt2_cost &lt;dbl&gt;, alt3_farm2 &lt;dbl&gt;,\n#   alt3_farm3 &lt;dbl&gt;, alt3_height2 &lt;dbl&gt;, alt3_height3 &lt;dbl&gt;,\n#   alt3_redkite &lt;dbl&gt;, alt3_distance &lt;dbl&gt;, alt3_cost &lt;dbl&gt;\n\n\nHere, we see that the blocking column is practically uncorrelated with the rest of the design."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Valuing Nature",
    "section": "",
    "text": "In this course, we begin each topic by reviewing the latest best practices for each method. While reading about how to estimate the value of nature is important, applying these methods in practice is half the battle. Throughout the course, we will periodically incorporate hands-on examples that demonstrate how to carry out these estimation techniques using real data.\nThis module is to help beginners with material on implementation of non-market valuation (NMV) with R (https://www.R-project.org). NMV methods have been widely applied in the social sciences such as environmental economics, agricultural economics, and transportation economics. Although various methods are associated with NMV, we focus on contingent valuation, discrete choice experiments, travel cost method, and hedonic pricing."
  },
  {
    "objectID": "index.html#what-is-r",
    "href": "index.html#what-is-r",
    "title": "Valuing Nature",
    "section": "What is R?",
    "text": "What is R?\nR is a programming language and software environment specifically designed for statistical computing and data analysis. It is widely used by statisticians, data scientists, researchers, and analysts to explore, model, and visualize data. Developed in the early 1990s, R has since become one of the most popular tools for applied statistics and quantitative analysis.\nR is open-source, meaning it’s free to use and supported by a large and active global community. Users can contribute packages to CRAN (the Comprehensive R Archive Network), which hosts thousands of libraries that extend R’s functionality across fields like economics, epidemiology, ecology, and more."
  },
  {
    "objectID": "index.html#download",
    "href": "index.html#download",
    "title": "Valuing Nature",
    "section": "Download",
    "text": "Download\nWe will be getting familiar with R (and RStudio, if necessary). If you are completely unfamiliar with R, you are advised to consult [An Introduction to R] (https://CRAN.R-project.org/manuals.html) or the other support material, which may be found on the websites mentioned below, before referring the contents.\n\nR Project (https://www.R-project.org/)\nThe Comprehensive R Archive Network (CRAN) (https://CRAN.R-project.org/)\nOfficial R Manuals on CRAN (https://CRAN.R-project.org/manuals.html)\nRStudio (https://www.RStudio.com/)"
  },
  {
    "objectID": "tc.html#map",
    "href": "tc.html#map",
    "title": "Travel Cost",
    "section": "Map",
    "text": "Map\n\n\nShow code\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(leaflet)\n\n# Safe extractors\nget_lon &lt;- function(x) {\n  if (is.null(x) || length(x) &lt; 2) return(NA_real_)\n  x[1]\n}\n\nget_lat &lt;- function(x) {\n  if (is.null(x) || length(x) &lt; 2) return(NA_real_)\n  x[2]\n}\n\n# Add lon/lat columns\nfacilities_df &lt;- facilities_df %&gt;%\n  mutate(\n    lon = map_dbl(GEOJSON.COORDINATES, get_lon),\n    lat = map_dbl(GEOJSON.COORDINATES, get_lat)\n  ) %&gt;%\n  filter(!is.na(lon), !is.na(lat))  # removes NULL rows\n\n# Build interactive map centered on Yosemite\nleaflet(facilities_df) %&gt;%\n  addProviderTiles(providers$Esri.NatGeoWorldMap) %&gt;%  # prettier tile layer\n  setView(lng = -119.5383, lat = 37.8651, zoom = 6) %&gt;%\n  addCircleMarkers(\n    lng = ~lon,\n    lat = ~lat,\n    radius = 6,\n    weight = 1,\n    color = \"darkgreen\",\n    fillColor = \"lightgreen\",\n    fillOpacity = 0.85,\n    popup = ~paste0(\n      \"&lt;b&gt;Location&lt;/b&gt;&lt;br&gt;\",\n      \"Site:\", FacilityName,\"&lt;br&gt;\",\n      \"&lt;b&gt;Coordinates&lt;/b&gt;&lt;br&gt;\",\n      \"Lat: \", round(lat, 5), \"&lt;br&gt;\",\n      \"Lon: \", round(lon, 5)\n       \n    )\n  )"
  },
  {
    "objectID": "tc.html#reservations",
    "href": "tc.html#reservations",
    "title": "Travel Cost",
    "section": "Reservations",
    "text": "Reservations"
  }
]